{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref: https://github.com/PratyushTripathy/Landsat-Classification-Using-Convolution-Neural-Network/tree/master\n",
    "\n",
    "https://github.com/weecology/DeepTreeAttention/blob/main/README.md (attention + pylighting fw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal\n",
    "from osgeo import gdalconst\n",
    "import os\n",
    "from osgeo import ogr\n",
    "from osgeo import osr\n",
    "import fiona\n",
    "from ops.ops import load_json\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from osgeo import gdal_array\n",
    "from skimage.morphology import disk, dilation, erosion\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref: \n",
    "https://github.com/klwalker-sb/burntfields_punjab\n",
    "\n",
    "https://github.com/aime-valentin/tree_species_predictions/tree/master\n",
    "\n",
    "https://github.com/swcoughlan/seaweed-classification\n",
    "\n",
    "https://github.com/MitaliBhurani/Delineating-urban-areas-from-satellite-imagery/blob/master/Sentinel_imbalaced_moradabad_cv.ipynb\n",
    "\n",
    "https://github.com/ML-MachineLearning/randomforest-GA/blob/master/random_forest.ipynb\n",
    "\n",
    "https://github.com/AgataKisel/imagery_classification-/blob/main/random_forest.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import rasterio\n",
    "raster_path = r'D:\\Sync\\research\\tree_species_estimation\\tree_dataset\\rmf\\processed\\20m\\labels\\tiles_128\\tile_0_31.tif'\n",
    "src  = rasterio.open(raster_path)\n",
    "src.nodata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import rasterio\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "class TreeSpeciesDataset(Dataset):\n",
    "    def __init__(self, tiles_dir, labels_dir, tile_names):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tiles_dir (str): Directory containing the input tiles (12 bands).\n",
    "            labels_dir (str): Directory containing the label tiles (9 bands).\n",
    "            tile_names (list): List of tile filenames to load (e.g., 'tile_x_y.tif').\n",
    "        \"\"\"\n",
    "        self.tiles_dir = tiles_dir\n",
    "        self.labels_dir = labels_dir\n",
    "        self.tile_names = tile_names\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tile_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tile_name = self.tile_names[idx]\n",
    "\n",
    "        # Load the input tile (12 bands)\n",
    "        input_tile_path = os.path.join(self.tiles_dir, tile_name)\n",
    "        with rasterio.open(input_tile_path) as src:\n",
    "            input_data = src.read()  # Shape: (12, H, W)\n",
    "            nodata_value_input = src.nodata  # NoData value for the input\n",
    "\n",
    "        # Load the target tile (9 bands)\n",
    "        label_tile_path = os.path.join(self.labels_dir, tile_name)\n",
    "        with rasterio.open(label_tile_path) as src:\n",
    "            target_data = src.read()  # Shape: (9, H, W)\n",
    "            nodata_value_target = src.nodata  # NoData value for the target\n",
    "\n",
    "        # Create masks for NoData pixels (True where NoData)\n",
    "        input_mask = input_data == nodata_value_input\n",
    "        target_mask = target_data == nodata_value_target\n",
    "\n",
    "        # Convert NoData to -1 in both input and target for easier handling\n",
    "        input_data = np.where(input_mask,-1, input_data)\n",
    "        target_data = np.where(target_mask, -1, target_data)\n",
    "\n",
    "        # Create a combined mask (where either input or target has NoData)\n",
    "        combined_mask = np.any(input_mask, axis=0) | np.any(target_mask, axis=0)\n",
    "\n",
    "        # Convert data to PyTorch tensors\n",
    "        input_tensor = torch.from_numpy(input_data).float()\n",
    "        target_tensor = torch.from_numpy(target_data).float()\n",
    "        mask_tensor = torch.from_numpy(combined_mask).bool()  # Convert mask to PyTorch boolean tensor\n",
    "\n",
    "        return input_tensor, target_tensor, mask_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=12, out_channels=9):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.enc_conv0 = nn.Conv2d(in_channels, 64, kernel_size=3, padding=1)\n",
    "        self.enc_conv1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.enc_conv2 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.enc_conv3 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        # Decoder\n",
    "        self.dec_conv3 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.dec_conv2 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.dec_conv1 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.dec_conv0 = nn.Conv2d(64, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        # Maxpool and upsample\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1 = F.relu(self.enc_conv0(x))\n",
    "        x2 = self.pool(x1)\n",
    "        x2 = F.relu(self.enc_conv1(x2))\n",
    "        x3 = self.pool(x2)\n",
    "        x3 = F.relu(self.enc_conv2(x3))\n",
    "        x4 = self.pool(x3)\n",
    "        x4 = F.relu(self.enc_conv3(x4))\n",
    "\n",
    "        # Decoder\n",
    "        x = self.up(x4)\n",
    "        x = F.relu(self.dec_conv3(x))\n",
    "        x = self.up(x)\n",
    "        x = F.relu(self.dec_conv2(x))\n",
    "        x = self.up(x)\n",
    "        x = F.relu(self.dec_conv1(x))\n",
    "        x = F.relu(self.dec_conv0(x))  # Output layer without activation\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def load_tile_names(file_path):\n",
    "    \"\"\"\n",
    "    Load tile names from a .txt file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the .txt file.\n",
    "\n",
    "    Returns:\n",
    "        tile_names (list): List of tile names.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        tile_names = f.read().splitlines()\n",
    "    return tile_names\n",
    "\n",
    "# Directories\n",
    "tiles_dir = \"s2/tiles_128\"  # Directory containing the 12-band Sentinel imagery\n",
    "labels_dir = \"labels/tiles_128\"  # Directory containing the 9-band species proportions\n",
    "\n",
    "# Load tile names for each split\n",
    "train_tile_names = load_tile_names(\"train.txt\")\n",
    "val_tile_names = load_tile_names(\"validation.txt\")\n",
    "test_tile_names = load_tile_names(\"test.txt\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TreeSpeciesDataset(tiles_dir, labels_dir, train_tile_names)\n",
    "val_dataset = TreeSpeciesDataset(tiles_dir, labels_dir, val_tile_names)\n",
    "test_dataset = TreeSpeciesDataset(tiles_dir, labels_dir, test_tile_names)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 4  # Adjust based on your memory capacity\n",
    "num_workers = 4  # Adjust based on your system\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = UNet(in_channels=12, out_channels=9)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 10  # Adjust as needed\n",
    "\n",
    "# Training loop\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, targets, mask in train_loader:\n",
    "        inputs = inputs.to(device)  # Shape: (batch_size, 12, H, W)\n",
    "        targets = targets.to(device)  # Shape: (batch_size, 9, H, W)\n",
    "        mask = mask.to(device)  # Shape: (batch_size, H, W)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)  # Shape: (batch_size, 9, H, W)\n",
    "\n",
    "        # Apply the mask to ignore NoData pixels in the loss calculation\n",
    "        valid_outputs = outputs[:, :, ~mask]\n",
    "        valid_targets = targets[:, :, ~mask]\n",
    "\n",
    "        # Compute loss only for valid pixels\n",
    "        loss = criterion(valid_outputs, valid_targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets, mask in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            mask = mask.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Apply the mask to ignore NoData pixels\n",
    "            valid_outputs = outputs[:, :, ~mask]\n",
    "            valid_targets = targets[:, :, ~mask]\n",
    "\n",
    "            loss = criterion(valid_outputs, valid_targets)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model_save_path = \"tree_species_model.pth\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing step\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pylighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class UNetLightning(pl.LightningModule):\n",
    "    def __init__(self, in_channels=12, out_channels=9, learning_rate=1e-3):\n",
    "        super(UNetLightning, self).__init__()\n",
    "\n",
    "        # Define the U-Net architecture\n",
    "        self.enc_conv0 = nn.Conv2d(in_channels, 64, kernel_size=3, padding=1)\n",
    "        self.enc_conv1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.enc_conv2 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.enc_conv3 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        self.dec_conv3 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.dec_conv2 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.dec_conv1 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.dec_conv0 = nn.Conv2d(64, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.criterion = nn.MSELoss()  # You can also experiment with other losses if needed\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1 = F.relu(self.enc_conv0(x))\n",
    "        x2 = self.pool(x1)\n",
    "        x2 = F.relu(self.enc_conv1(x2))\n",
    "        x3 = self.pool(x2)\n",
    "        x3 = F.relu(self.enc_conv2(x3))\n",
    "        x4 = self.pool(x3)\n",
    "        x4 = F.relu(self.enc_conv3(x4))\n",
    "\n",
    "        # Decoder\n",
    "        x = self.up(x4)\n",
    "        x = F.relu(self.dec_conv3(x))\n",
    "        x = self.up(x)\n",
    "        x = F.relu(self.dec_conv2(x))\n",
    "        x = self.up(x)\n",
    "        x = F.relu(self.dec_conv1(x))\n",
    "        x = self.dec_conv0(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, targets, mask = batch\n",
    "        outputs = self(inputs)\n",
    "\n",
    "        # Apply the mask to exclude NoData pixels\n",
    "        valid_outputs = outputs[:, :, ~mask]\n",
    "        valid_targets = targets[:, :, ~mask]\n",
    "\n",
    "        # Compute the loss only for valid pixels\n",
    "        loss = self.criterion(valid_outputs, valid_targets)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, targets, mask = batch\n",
    "        outputs = self(inputs)\n",
    "\n",
    "        # Apply the mask to exclude NoData pixels\n",
    "        valid_outputs = outputs[:, :, ~mask]\n",
    "        valid_targets = targets[:, :, ~mask]\n",
    "\n",
    "        loss = self.criterion(valid_outputs, valid_targets)\n",
    "        self.log('val_loss', loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class TreeSpeciesDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, tiles_dir, labels_dir, train_tile_names, val_tile_names, test_tile_names, batch_size=4, num_workers=4):\n",
    "        super().__init__()\n",
    "        self.tiles_dir = tiles_dir\n",
    "        self.labels_dir = labels_dir\n",
    "        self.train_tile_names = train_tile_names\n",
    "        self.val_tile_names = val_tile_names\n",
    "        self.test_tile_names = test_tile_names\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Create datasets for train, val, test splits\n",
    "        self.train_dataset = TreeSpeciesDataset(self.tiles_dir, self.labels_dir, self.train_tile_names)\n",
    "        self.val_dataset = TreeSpeciesDataset(self.tiles_dir, self.labels_dir, self.val_tile_names)\n",
    "        self.test_dataset = TreeSpeciesDataset(self.tiles_dir, self.labels_dir, self.test_tile_names)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "# Initialize the data module\n",
    "data_module = TreeSpeciesDataModule(\n",
    "    tiles_dir=\"s2/tiles_128\",\n",
    "    labels_dir=\"labels/tiles_128\",\n",
    "    train_tile_names=load_tile_names(\"train.txt\"),\n",
    "    val_tile_names=load_tile_names(\"validation.txt\"),\n",
    "    test_tile_names=load_tile_names(\"test.txt\"),\n",
    "    batch_size=4,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Initialize the model\n",
    "model = UNetLightning(in_channels=12, out_channels=9, learning_rate=1e-3)\n",
    "\n",
    "# Define a checkpoint callback to save the best model\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',  # Track the validation loss\n",
    "    filename='best-model-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=1,  # Only save the best model\n",
    "    mode='min'  # We want to minimize the validation loss\n",
    ")\n",
    "\n",
    "# Create a PyTorch Lightning Trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    gpus=1 if torch.cuda.is_available() else 0,  # Use GPU if available\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "# Test the model after training\n",
    "trainer.test(model, data_module)\n",
    "\n",
    "# Save the best model after training\n",
    "trainer.save_checkpoint(\"final_model.ckpt\")\n",
    "# Load the saved model\n",
    "#model = UNetLightning.load_from_checkpoint(\"final_model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rf\n",
    "\n",
    "ref: https://github.com/shelleygoel/sentinel2-land-cover-classifier/tree/main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Key Points:\n",
    "  - X (Features): Sentinel imagery tiles stored in s2/tiles_128/ (each tile has 12 bands, size 128x128).\n",
    "  - Y (Labels): The species composition tiles stored in labels/tiles_128/ (each tile has 9 bands, size 128x128). The target for each pixel is a 9-element vector representing species proportions.\n",
    "  - Train/Validation/Test Splits: The tiles to use for training, validation, and testing are specified in train.txt, validation.txt, and test.txt.\n",
    "- Step-by-Step Implementation:\n",
    "  - Loading Data: We'll read all 1060 tiles from the directories for both input (X) and target (Y).\n",
    "  - Random Forest: We'll use RandomForestRegressor to fit the data.\n",
    "  - Training/Validation/Test Splits: These splits are defined by the .txt files.\n",
    "  - Pixel-Wise Classification: The model will predict the species proportions for each pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [01:40<00:00,  3.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of labels: (6553600, 12)\n",
      "shape of labels: (6553600, 9)\n",
      "loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:34<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of labels: (1638400, 12)\n",
      "shape of labels: (1638400, 9)\n",
      "start training...\n",
      "validating...\n",
      "Validation Mean Squared Error: 0.21201176008304376\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to load tiles (X) and labels (Y)\n",
    "def load_tile_data(tile_names, tiles_dir, labels_dir):\n",
    "    \"\"\"\n",
    "    Load the imagery (X) and label (Y) data for the given tile names.\n",
    "\n",
    "    Args:\n",
    "        tile_names (list): List of tile names to load.\n",
    "        tiles_dir (str): Directory containing the Sentinel imagery (X).\n",
    "        labels_dir (str): Directory containing the species composition labels (Y).\n",
    "\n",
    "    Returns:\n",
    "        X (np.array): Flattened feature array (pixels x 12).\n",
    "        Y (np.array): Flattened label array (pixels x 9).\n",
    "    \"\"\"\n",
    "    X_list, Y_list = [], []\n",
    "    print(\"loading data...\")\n",
    "    for tile_name in tqdm(tile_names):\n",
    "        # Define paths for the input and label tiles\n",
    "        input_tile_path = os.path.join(tiles_dir, tile_name)\n",
    "        label_tile_path = os.path.join(labels_dir, tile_name)\n",
    "        \n",
    "        # Load input (12 bands) and label (9 bands) tiles\n",
    "        with rasterio.open(input_tile_path) as src_x:\n",
    "            X = src_x.read()  # Shape: (12, 128, 128)\n",
    "\n",
    "        with rasterio.open(label_tile_path) as src_y:\n",
    "            Y = src_y.read()  # Shape: (9, 128, 128)\n",
    "\n",
    "        # Reshape to (num_pixels, num_bands)\n",
    "        X_flat = X.reshape(X.shape[0], -1).T  # Shape: (num_pixels, 12)\n",
    "        Y_flat = Y.reshape(Y.shape[0], -1).T  # Shape: (num_pixels, 9)\n",
    "\n",
    "        # Append to lists\n",
    "        X_list.append(X_flat)\n",
    "        Y_list.append(Y_flat)\n",
    "    \n",
    "    # Concatenate all tiles into a single array\n",
    "    X_all = np.vstack(X_list)\n",
    "    print(f\"shape of labels: {X_all.shape}\")\n",
    "    Y_all = np.vstack(Y_list)\n",
    "    print(f\"shape of labels: {Y_all.shape}\")\n",
    "    \n",
    "    return X_all, Y_all\n",
    "\n",
    "# Function to read the train/validation/test splits\n",
    "def load_split(file_path):\n",
    "    \"\"\"\n",
    "    Load the tile names from the train/validation/test split files.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the split .txt file.\n",
    "\n",
    "    Returns:\n",
    "        tile_names (list): List of tile names in the split.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        tile_names = file.read().splitlines()\n",
    "    return tile_names\n",
    "\n",
    "# Set up directories\n",
    "directory = r\"D:\\Sync\\research\\tree_species_estimation\\tree_dataset\\rmf\\processed\\10m\"\n",
    "tiles_dir = os.path.join(directory, \"rmf_s2\", \"summer\", \"tiles_128\")  # Directory for X\n",
    "labels_dir = os.path.join(directory, \"labels\", \"tiles_128\")  # Directory for Y\n",
    "\n",
    "# Load train/validation/test splits\n",
    "train_tile_names = load_split(os.path.join(directory, \"dataset\", \"train_tiles.txt\"))[:400]\n",
    "val_tile_names = load_split(os.path.join(directory, \"dataset\", \"val_tiles.txt\"))[:100]\n",
    "\n",
    "# Load the training data\n",
    "X_train, Y_train = load_tile_data(train_tile_names, tiles_dir, labels_dir)\n",
    "\n",
    "# Load the validation data (optional, but useful for hyperparameter tuning)\n",
    "X_val, Y_val = load_tile_data(val_tile_names, tiles_dir, labels_dir)\n",
    "\n",
    "# Initialize and train the Random Forest model\n",
    "print(\"start training...\")\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, Y_train)\n",
    "\n",
    "# Evaluate on the validation set\n",
    "print(\"validating...\")\n",
    "Y_val_pred = rf.predict(X_val)\n",
    "val_mse = mean_squared_error(Y_val, Y_val_pred)\n",
    "print(f\"Validation Mean Squared Error: {val_mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# After training the model\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, Y_train)\n",
    "\n",
    "# Save the model to a file\n",
    "model_filename = 'random_forest_model.joblib'\n",
    "joblib.dump(rf, model_filename)\n",
    "\n",
    "print(f\"Model saved as {model_filename}\")\n",
    "\n",
    "# Load the saved model\n",
    "loaded_rf = joblib.load(model_filename)\n",
    "\n",
    "# Load the testing data (for final evaluation)\n",
    "test_tile_names = load_split(os.path.join(directory, \"dataset\", \"test_tiles.txt\"))\n",
    "X_test, Y_test = load_tile_data(test_tile_names, tiles_dir, labels_dir)\n",
    "\n",
    "# Use the loaded model to make predictions\n",
    "Y_test_pred = loaded_rf.predict(X_test)\n",
    "test_mse = mean_squared_error(Y_test, Y_test_pred)\n",
    "print(f\"Test Mean Squared Error with loaded model: {test_mse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LUCinSA_helpers\n",
    "Helper functions and notebooks to interact with data on High-Performance Computing environment, designed to be used in conjunction with processing guide for remote sensing projects on Land-Use Change in Latin America:\n",
    "\n",
    "https://github.com/klwalker-sb/LUCinSA_helpers/tree/master\n",
    "\n",
    "https://klwalker-sb.github.io/LUCinLA_stac/Downloading.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
