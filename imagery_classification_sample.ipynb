{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref: https://github.com/PratyushTripathy/Landsat-Classification-Using-Convolution-Neural-Network/tree/master\n",
    "\n",
    "https://github.com/weecology/DeepTreeAttention/blob/main/README.md (attention + pylighting fw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train/test/val splits + balance dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from label_gen import get_tile_names_from_folder, load_raster_data_from_tiles, iterative_split, save_tile_names\n",
    "# Generate dataset splits\n",
    "resolutions = [10, 20]\n",
    "directory = r\"D:\\Sync\\research\\tree_species_estimation\\tree_dataset\\rmf\\processed\"\n",
    "for resolution in resolutions:\n",
    "    input_folder = os.path.join(directory, f\"{resolution}m\", \"labels\", \"tiles_128\")\n",
    "    output_folder = os.path.join(directory, f\"{resolution}m\", \"dataset\")\n",
    "\n",
    "    # Step 1: Get tile names from the input folder\n",
    "    tile_names = get_tile_names_from_folder(input_folder)\n",
    "\n",
    "    # Step 2: Load the actual raster data from tiles\n",
    "    raster_data = load_raster_data_from_tiles(input_folder, tile_names)\n",
    "\n",
    "    # Step 3: Perform the iterative split\n",
    "    train_indices, val_indices, test_indices = iterative_split(raster_data)\n",
    "\n",
    "    # Step 4: Save the tile names into .txt files\n",
    "    save_tile_names(tile_names, train_indices, val_indices, test_indices, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal\n",
    "from osgeo import gdalconst\n",
    "import os\n",
    "from osgeo import ogr\n",
    "from osgeo import osr\n",
    "import fiona\n",
    "from ops.ops import load_json\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from osgeo import gdal_array\n",
    "from skimage.morphology import disk, dilation, erosion\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref: \n",
    "https://github.com/klwalker-sb/burntfields_punjab\n",
    "\n",
    "https://github.com/aime-valentin/tree_species_predictions/tree/master\n",
    "\n",
    "https://github.com/swcoughlan/seaweed-classification\n",
    "\n",
    "https://github.com/MitaliBhurani/Delineating-urban-areas-from-satellite-imagery/blob/master/Sentinel_imbalaced_moradabad_cv.ipynb\n",
    "\n",
    "https://github.com/ML-MachineLearning/randomforest-GA/blob/master/random_forest.ipynb\n",
    "\n",
    "https://github.com/AgataKisel/imagery_classification-/blob/main/random_forest.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imagery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pylighting - UNET code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "TreeSpeciesDataModule\n",
    "\n",
    "              ↓\n",
    " (inputs, targets, masks)  ← from DataLoader\n",
    "\n",
    "               ↓\n",
    " Training Loop\n",
    "\n",
    "              ↓\n",
    " MaskedMSELoss(outputs, targets, masks)\n",
    " \n",
    "              ↓\n",
    " Backpropagation (only for valid pixels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "import os\n",
    "import rasterio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Masked MSE Loss\n",
    "class MaskedMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaskedMSELoss, self).__init__()\n",
    "\n",
    "    def forward(self, outputs, targets, mask):\n",
    "        \"\"\"\n",
    "        Custom MSE loss function that ignores NoData pixels.\n",
    "\n",
    "        Args:\n",
    "        - outputs: Predicted values (batch_size, num_channels, H, W)\n",
    "        - targets: Ground truth values (batch_size, num_channels, H, W)\n",
    "        - mask: Boolean mask indicating NoData pixels (batch_size, H, W)\n",
    "\n",
    "        Returns:\n",
    "        - loss: Mean squared error computed only for valid pixels.\n",
    "        \"\"\"\n",
    "        # Expand mask to have the same number of channels as outputs and targets\n",
    "        expanded_mask = mask.unsqueeze(1).expand_as(outputs)  # Shape: (batch_size, num_channels, H, W)\n",
    "        \n",
    "        # Compute squared difference between outputs and targets\n",
    "        diff = (outputs - targets) ** 2\n",
    "\n",
    "        # Zero out contributions from NoData pixels (where mask is True)\n",
    "        diff = diff * (~expanded_mask)  # Keep valid pixels only\n",
    "\n",
    "        # Sum over the channel and spatial dimensions (H, W)\n",
    "        loss = diff.sum(dim=(1, 2, 3))\n",
    "\n",
    "        # Count the number of valid pixels per batch (sum of ~mask)\n",
    "        num_valid_pixels = (~expanded_mask).sum(dim=(1, 2, 3)).float()\n",
    "\n",
    "        # Prevent division by zero (in case all pixels are NoData)\n",
    "        num_valid_pixels = torch.clamp(num_valid_pixels, min=1.0)\n",
    "\n",
    "        # Compute the mean loss per valid pixel\n",
    "        loss = loss / num_valid_pixels\n",
    "\n",
    "        # Return the mean loss over the batch\n",
    "        return loss.mean()\n",
    "\n",
    "def r2_score_torch(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the R² score in PyTorch to avoid moving tensors to CPU.\n",
    "    \n",
    "    Args:\n",
    "    - y_true: Ground truth tensor (valid pixels, num_channels).\n",
    "    - y_pred: Predicted tensor (valid pixels, num_channels).\n",
    "\n",
    "    Returns:\n",
    "    - r2: The R² score computed in PyTorch.\n",
    "    \"\"\"\n",
    "    # Mean of the true values\n",
    "    y_true_mean = torch.mean(y_true, dim=0)\n",
    "\n",
    "    # Total sum of squares (TSS)\n",
    "    total_variance = torch.sum((y_true - y_true_mean) ** 2, dim=0)\n",
    "\n",
    "    # Residual sum of squares (RSS)\n",
    "    residuals = torch.sum((y_true - y_pred) ** 2, dim=0)\n",
    "\n",
    "    # Compute R² score for each channel and take mean\n",
    "    r2 = 1 - (residuals / total_variance)\n",
    "    return r2.mean()  # Mean R² across all channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Lightning Module\n",
    "class UNetLightning(pl.LightningModule):\n",
    "    def __init__(self, in_channels, out_channels=9, learning_rate=1e-3):\n",
    "        super(UNetLightning, self).__init__()\n",
    "        \n",
    "        # Define the U-Net architecture\n",
    "        self.enc_conv0 = nn.Conv2d(in_channels, 64, kernel_size=3, padding=1)\n",
    "        self.enc_conv1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.enc_conv2 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.enc_conv3 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        self.dec_conv3 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.dec_conv2 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.dec_conv1 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.dec_conv0 = nn.Conv2d(64, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Use the MaskedMSELoss\n",
    "        self.criterion = MaskedMSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1 = F.relu(self.enc_conv0(x))\n",
    "        x2 = self.pool(x1)\n",
    "        x2 = F.relu(self.enc_conv1(x2))\n",
    "        x3 = self.pool(x2)\n",
    "        x3 = F.relu(self.enc_conv2(x3))\n",
    "        x4 = self.pool(x3)\n",
    "        x4 = F.relu(self.enc_conv3(x4))\n",
    "\n",
    "        # Decoder\n",
    "        x = self.up(x4)\n",
    "        x = F.relu(self.dec_conv3(x))\n",
    "        x = self.up(x)\n",
    "        x = F.relu(self.dec_conv2(x))\n",
    "        x = self.up(x)\n",
    "        x = F.relu(self.dec_conv1(x))\n",
    "        x = self.dec_conv0(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, targets, masks = batch\n",
    "        outputs = self(inputs)  # Forward pass\n",
    "        # Expand the mask to match the number of channels in outputs and targets\n",
    "        expanded_mask = masks.unsqueeze(1).expand_as(outputs)  # Shape: (batch_size, num_channels, H, W)\n",
    "\n",
    "        # Exclude NoData pixels by applying the mask (keep only valid pixels)\n",
    "        valid_outputs = outputs.masked_select(~expanded_mask).view(-1, outputs.size(1))\n",
    "        valid_targets = targets.masked_select(~expanded_mask).view(-1, targets.size(1))\n",
    "\n",
    "        # Compute the masked loss\n",
    "        loss = self.criterion(outputs, targets, masks)\n",
    "        # Calculate R² score for valid pixels\n",
    "        r2 = r2_score_torch(valid_targets, valid_outputs)  # R² calculated in PyTorch\n",
    "\n",
    "        # Log the training loss and R² score\n",
    "        self.log('train_loss', loss, logger=True)\n",
    "        self.log('train_r2', r2, logger=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, targets, masks = batch\n",
    "        outputs = self(inputs)  # Forward pass\n",
    "        # Expand the mask to match the number of channels in outputs and targets\n",
    "        expanded_mask = masks.unsqueeze(1).expand_as(outputs)  # Shape: (batch_size, num_channels, H, W)\n",
    "\n",
    "        # Exclude NoData pixels by applying the mask (keep only valid pixels)\n",
    "        valid_outputs = outputs.masked_select(~expanded_mask).view(-1, outputs.size(1))\n",
    "        valid_targets = targets.masked_select(~expanded_mask).view(-1, targets.size(1))\n",
    "        \n",
    "        # Compute the masked loss\n",
    "        loss = self.criterion(outputs, targets, masks)\n",
    "        # Calculate R² score for valid pixels\n",
    "        r2 = r2_score_torch(valid_targets, valid_outputs)  # R² calculated in PyTorch\n",
    "\n",
    "        # Log the validation loss and R² score\n",
    "        self.log('val_loss', loss, logger=True)\n",
    "        self.log('val_r2', r2, logger=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        inputs, targets, masks = batch\n",
    "        outputs = self(inputs)  # Forward pass\n",
    "\n",
    "        expanded_mask = masks.unsqueeze(1).expand_as(outputs)\n",
    "        valid_outputs = outputs.masked_select(~expanded_mask).view(-1, outputs.size(1))\n",
    "        valid_targets = targets.masked_select(~expanded_mask).view(-1, targets.size(1))\n",
    "        \n",
    "        # Compute the masked loss\n",
    "        loss = self.criterion(outputs, targets, masks)\n",
    "        \n",
    "        # Calculate R² score for valid pixels\n",
    "        r2 = r2_score_torch(valid_targets, valid_outputs)  # R² calculated in PyTorch\n",
    "\n",
    "        # Log the test loss and R² score\n",
    "        self.log('test_loss', loss, logger=True)\n",
    "        self.log('test_r2', r2, logger=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeSpeciesDataset(Dataset):\n",
    "    def __init__(self, tile_names, processed_dir, datasets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tile_names (list): List of tile filenames to load.\n",
    "            processed_dir (str): Base directory containing the processed data folders.\n",
    "            datasets (list): List of dataset folder names to include (e.g., ['s2/spring', 's2/summer', 'topo', 'climate']).\n",
    "        \"\"\"\n",
    "        self.tile_names = tile_names\n",
    "        self.processed_dir = processed_dir\n",
    "        self.datasets = datasets  # List of dataset folder names\n",
    "\n",
    "        # Calculate total input channels automatically\n",
    "        self.total_input_channels = self.calculate_total_input_channels()\n",
    "\n",
    "    def calculate_total_input_channels(self):\n",
    "        \"\"\"\n",
    "        Calculate the total number of input channels by inspecting one file from each dataset.\n",
    "        \"\"\"\n",
    "        total_channels = 0\n",
    "        for dataset in self.datasets:\n",
    "            example_file = os.path.join(self.processed_dir, dataset, self.tile_names[0])  # Use first tile to inspect\n",
    "            with rasterio.open(example_file) as src:\n",
    "                total_channels += src.count  # Add the number of bands in the dataset\n",
    "        return total_channels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tile_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tile_name = self.tile_names[idx]\n",
    "        input_data_list = []\n",
    "\n",
    "        # Load data from each dataset (spring, summer, topo, climate, etc.)\n",
    "        for dataset in self.datasets:\n",
    "            dataset_path = os.path.join(self.processed_dir, dataset, tile_name)\n",
    "            with rasterio.open(dataset_path) as src:\n",
    "                input_data = src.read()  # Read the bands (num_bands, H, W)\n",
    "                input_data_list.append(input_data)\n",
    "\n",
    "        # Combine all the input data into a single input tensor\n",
    "        input_data = np.concatenate(input_data_list, axis=0)  # Concatenate along the channel axis\n",
    "\n",
    "        # Load the corresponding label (target species composition)\n",
    "        label_path = os.path.join(self.processed_dir, 'labels/tiles_128', tile_name)\n",
    "        with rasterio.open(label_path) as src:\n",
    "            target_data = src.read()  # (num_bands, H, W)\n",
    "            nodata_value_label = src.nodata  # NoData value for the labels\n",
    "\n",
    "            # Create a NoData mask for the target data\n",
    "            if nodata_value_label is not None:\n",
    "                mask = np.any(target_data == nodata_value_label, axis=0)  # Collapse bands to (H, W)\n",
    "            else:\n",
    "                mask = np.zeros_like(target_data[0], dtype=bool)  # Assume all valid if no NoData value\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        input_tensor = torch.from_numpy(input_data).float()  # Shape: (total_input_channels, H, W)\n",
    "        target_tensor = torch.from_numpy(target_data).float()  # Shape: (num_output_channels, H, W)\n",
    "        mask_tensor = torch.from_numpy(mask).bool()  # Shape: (H, W), NoData mask\n",
    "\n",
    "        return input_tensor, target_tensor, mask_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeSpeciesDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, tile_names, processed_dir, datasets_to_use, batch_size=4, num_workers=4):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tile_names (list): List of tile filenames to load.\n",
    "            processed_dir (str): Directory where processed data is located.\n",
    "            datasets_to_use (list): List of dataset names to include (e.g., ['s2/spring', 's2/summer', 'topo']).\n",
    "            batch_size (int): Batch size for DataLoader.\n",
    "            num_workers (int): Number of workers for DataLoader.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.tile_names = tile_names\n",
    "        self.processed_dir = processed_dir\n",
    "        self.datasets_to_use = datasets_to_use\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "        # Calculate total input channels based on the datasets\n",
    "        temp_dataset = TreeSpeciesDataset(self.tile_names['train'], self.processed_dir, self.datasets_to_use)\n",
    "        self.input_channels = temp_dataset.total_input_channels\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        \"\"\"\n",
    "        Sets up the dataset for train, validation, and test splits.\n",
    "        \"\"\"\n",
    "        # Create datasets for train, validation, and test\n",
    "        self.train_dataset = TreeSpeciesDataset(self.tile_names['train'], self.processed_dir, self.datasets_to_use)\n",
    "        self.val_dataset = TreeSpeciesDataset(self.tile_names['val'], self.processed_dir, self.datasets_to_use)\n",
    "        self.test_dataset = TreeSpeciesDataset(self.tile_names['test'], self.processed_dir, self.datasets_to_use)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tile_names(file_path):\n",
    "    \"\"\"\n",
    "    Load tile names from a .txt file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the .txt file.\n",
    "\n",
    "    Returns:\n",
    "        tile_names (list): List of tile names.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        tile_names = f.read().splitlines()\n",
    "    return tile_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type          | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0  | enc_conv0 | Conv2d        | 5.2 K  | train\n",
      "1  | enc_conv1 | Conv2d        | 73.9 K | train\n",
      "2  | enc_conv2 | Conv2d        | 295 K  | train\n",
      "3  | enc_conv3 | Conv2d        | 1.2 M  | train\n",
      "4  | dec_conv3 | Conv2d        | 1.2 M  | train\n",
      "5  | dec_conv2 | Conv2d        | 295 K  | train\n",
      "6  | dec_conv1 | Conv2d        | 73.8 K | train\n",
      "7  | dec_conv0 | Conv2d        | 5.2 K  | train\n",
      "8  | pool      | MaxPool2d     | 0      | train\n",
      "9  | up        | Upsample      | 0      | train\n",
      "10 | criterion | MaskedMSELoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "3.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 M     Total params\n",
      "12.433    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 186/186 [00:19<00:00,  9.33it/s, v_num=26, train_r2=0.255, val_r2=0.366]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 186/186 [00:19<00:00,  9.33it/s, v_num=26, train_r2=0.255, val_r2=0.366]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 40/40 [00:03<00:00, 11.61it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.03386075049638748    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_r2          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.3600727319717407     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.03386075049638748   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test_r2         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.3600727319717407    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "# User specifies which datasets to use\n",
    "datasets_to_use = ['rmf_s2/fall/tiles_128']\n",
    "\n",
    "# Tile names for train, validation, and test\n",
    "tile_names = {\n",
    "    'train': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/train_tiles.txt\"),\n",
    "    'val': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/val_tiles.txt\"),\n",
    "    'test': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/test_tiles.txt\")\n",
    "}\n",
    "\n",
    "# Initialize the DataModule\n",
    "data_module = TreeSpeciesDataModule(\n",
    "    tile_names=tile_names,\n",
    "    processed_dir=\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m\",  # Base directory where the datasets are stored\n",
    "    datasets_to_use=datasets_to_use,\n",
    "    batch_size=4,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Use the calculated input channels from the DataModule to initialize the model\n",
    "model = UNetLightning(in_channels=data_module.input_channels, learning_rate=1e-3)\n",
    "\n",
    "# Define a checkpoint callback to save the best model\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',  # Track the validation loss\n",
    "    filename='best-model-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=1,  # Only save the best model\n",
    "    mode='min'  # We want to minimize the validation loss\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(save_dir='logs', name=\"unet_s2_loss_r2\")\n",
    "\n",
    "# Create a PyTorch Lightning Trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    logger=[csv_logger],\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "# Test the model after training\n",
    "trainer.test(model, data_module)\n",
    "\n",
    "# Save the best model after training\n",
    "trainer.save_checkpoint(\"final_model.ckpt\")\n",
    "# Load the saved model\n",
    "#model = UNetLightning.load_from_checkpoint(\"final_model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type          | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0  | enc_conv0 | Conv2d        | 7.0 K  | train\n",
      "1  | enc_conv1 | Conv2d        | 73.9 K | train\n",
      "2  | enc_conv2 | Conv2d        | 295 K  | train\n",
      "3  | enc_conv3 | Conv2d        | 1.2 M  | train\n",
      "4  | dec_conv3 | Conv2d        | 1.2 M  | train\n",
      "5  | dec_conv2 | Conv2d        | 295 K  | train\n",
      "6  | dec_conv1 | Conv2d        | 73.8 K | train\n",
      "7  | dec_conv0 | Conv2d        | 5.2 K  | train\n",
      "8  | pool      | MaxPool2d     | 0      | train\n",
      "9  | up        | Upsample      | 0      | train\n",
      "10 | criterion | MaskedMSELoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "3.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 M     Total params\n",
      "12.440    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 713/713 [01:18<00:00,  9.05it/s, v_num=27, train_r2=0.700, val_r2=0.447]    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 713/713 [01:19<00:00,  9.01it/s, v_num=27, train_r2=0.700, val_r2=0.447]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 153/153 [01:10<00:00,  2.17it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.029438093304634094    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_r2          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.4821441173553467     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.029438093304634094   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test_r2         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.4821441173553467    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "# User specifies which datasets to use\n",
    "datasets_to_use = ['rmf_s2/fall/tiles_128']\n",
    "\n",
    "# Tile names for train, validation, and test\n",
    "tile_names = {\n",
    "    'train': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/10m/dataset/train_tiles.txt\"),\n",
    "    'val': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/10m/dataset/val_tiles.txt\"),\n",
    "    'test': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/10m/dataset/test_tiles.txt\")\n",
    "}\n",
    "\n",
    "# Initialize the DataModule\n",
    "data_module = TreeSpeciesDataModule(\n",
    "    tile_names=tile_names,\n",
    "    processed_dir=\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/10m\",  # Base directory where the datasets are stored\n",
    "    datasets_to_use=datasets_to_use,\n",
    "    batch_size=4,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Use the calculated input channels from the DataModule to initialize the model\n",
    "model = UNetLightning(in_channels=data_module.input_channels, learning_rate=1e-3)\n",
    "\n",
    "# Define a checkpoint callback to save the best model\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',  # Track the validation loss\n",
    "    filename='best-model-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=1,  # Only save the best model\n",
    "    mode='min'  # We want to minimize the validation loss\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(save_dir='logs', name=\"unet_s2_loss_r2\")\n",
    "\n",
    "# Create a PyTorch Lightning Trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    logger=[csv_logger],\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "# Test the model after training\n",
    "trainer.test(model, data_module)\n",
    "\n",
    "# Save the best model after training\n",
    "trainer.save_checkpoint(\"final_model.ckpt\")\n",
    "# Load the saved model\n",
    "#model = UNetLightning.load_from_checkpoint(\"final_model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### summer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA RTX A4000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type          | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0  | enc_conv0 | Conv2d        | 5.2 K  | train\n",
      "1  | enc_conv1 | Conv2d        | 73.9 K | train\n",
      "2  | enc_conv2 | Conv2d        | 295 K  | train\n",
      "3  | enc_conv3 | Conv2d        | 1.2 M  | train\n",
      "4  | dec_conv3 | Conv2d        | 1.2 M  | train\n",
      "5  | dec_conv2 | Conv2d        | 295 K  | train\n",
      "6  | dec_conv1 | Conv2d        | 73.8 K | train\n",
      "7  | dec_conv0 | Conv2d        | 5.2 K  | train\n",
      "8  | pool      | MaxPool2d     | 0      | train\n",
      "9  | up        | Upsample      | 0      | train\n",
      "10 | criterion | MaskedMSELoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "3.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 M     Total params\n",
      "12.433    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 186/186 [00:18<00:00, 10.13it/s, v_num=1, train_r2=-0.273, val_r2=0.373]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 186/186 [00:18<00:00,  9.85it/s, v_num=1, train_r2=-0.273, val_r2=0.373]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing DataLoader 0: 100%|██████████| 40/40 [00:21<00:00,  1.83it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.03334387391805649    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_r2          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.36800849437713623    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.03334387391805649   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test_r2         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.36800849437713623   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "# Initialize the data module\n",
    "data_module = TreeSpeciesDataModule(\n",
    "    tiles_dir=\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/rmf_s2/summer/tiles_128\",\n",
    "    labels_dir=\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/labels/tiles_128\",\n",
    "    train_tile_names=load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/train_tiles.txt\"),\n",
    "    val_tile_names=load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/val_tiles.txt\"),\n",
    "    test_tile_names=load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/test_tiles.txt\"),\n",
    "    batch_size=4,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Initialize the model\n",
    "model = UNetLightning(in_channels=9, out_channels=9, learning_rate=1e-3)\n",
    "\n",
    "# Define a checkpoint callback to save the best model\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',  # Track the validation loss\n",
    "    filename='best-model-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=1,  # Only save the best model\n",
    "    mode='min'  # We want to minimize the validation loss\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(save_dir='logs', name=\"unet_s2_loss_r2\")\n",
    "\n",
    "# Create a PyTorch Lightning Trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    logger=[csv_logger],\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "# Test the model after training\n",
    "trainer.test(model, data_module)\n",
    "\n",
    "# Save the best model after training\n",
    "trainer.save_checkpoint(\"final_model.ckpt\")\n",
    "# Load the saved model\n",
    "#model = UNetLightning.load_from_checkpoint(\"final_model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type          | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0  | enc_conv0 | Conv2d        | 7.0 K  | train\n",
      "1  | enc_conv1 | Conv2d        | 73.9 K | train\n",
      "2  | enc_conv2 | Conv2d        | 295 K  | train\n",
      "3  | enc_conv3 | Conv2d        | 1.2 M  | train\n",
      "4  | dec_conv3 | Conv2d        | 1.2 M  | train\n",
      "5  | dec_conv2 | Conv2d        | 295 K  | train\n",
      "6  | dec_conv1 | Conv2d        | 73.8 K | train\n",
      "7  | dec_conv0 | Conv2d        | 5.2 K  | train\n",
      "8  | pool      | MaxPool2d     | 0      | train\n",
      "9  | up        | Upsample      | 0      | train\n",
      "10 | criterion | MaskedMSELoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "3.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 M     Total params\n",
      "12.440    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 713/713 [01:19<00:00,  8.92it/s, v_num=41, train_r2=0.503, val_r2=0.416]    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 713/713 [01:19<00:00,  8.92it/s, v_num=41, train_r2=0.503, val_r2=0.416]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 153/153 [01:09<00:00,  2.21it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.031047677621245384    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_r2          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.442976176738739     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.031047677621245384   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test_r2         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.442976176738739    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "# User specifies which datasets to use\n",
    "datasets_to_use = ['rmf_s2/summer/tiles_128']\n",
    "\n",
    "# Tile names for train, validation, and test\n",
    "tile_names = {\n",
    "    'train': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/10m/dataset/train_tiles.txt\"),\n",
    "    'val': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/10m/dataset/val_tiles.txt\"),\n",
    "    'test': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/10m/dataset/test_tiles.txt\")\n",
    "}\n",
    "\n",
    "# Initialize the DataModule\n",
    "data_module = TreeSpeciesDataModule(\n",
    "    tile_names=tile_names,\n",
    "    processed_dir=\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/10m\",  # Base directory where the datasets are stored\n",
    "    datasets_to_use=datasets_to_use,\n",
    "    batch_size=4,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Use the calculated input channels from the DataModule to initialize the model\n",
    "model = UNetLightning(in_channels=data_module.input_channels, learning_rate=1e-3)\n",
    "\n",
    "# Define a checkpoint callback to save the best model\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',  # Track the validation loss\n",
    "    filename='best-model-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=1,  # Only save the best model\n",
    "    mode='min'  # We want to minimize the validation loss\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(save_dir='logs', name=\"unet_s2_loss_r2\")\n",
    "\n",
    "# Create a PyTorch Lightning Trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    logger=[csv_logger],\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "# Test the model after training\n",
    "trainer.test(model, data_module)\n",
    "\n",
    "# Save the best model after training\n",
    "trainer.save_checkpoint(\"final_model.ckpt\")\n",
    "# Load the saved model\n",
    "#model = UNetLightning.load_from_checkpoint(\"final_model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type          | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0  | enc_conv0 | Conv2d        | 5.2 K  | train\n",
      "1  | enc_conv1 | Conv2d        | 73.9 K | train\n",
      "2  | enc_conv2 | Conv2d        | 295 K  | train\n",
      "3  | enc_conv3 | Conv2d        | 1.2 M  | train\n",
      "4  | dec_conv3 | Conv2d        | 1.2 M  | train\n",
      "5  | dec_conv2 | Conv2d        | 295 K  | train\n",
      "6  | dec_conv1 | Conv2d        | 73.8 K | train\n",
      "7  | dec_conv0 | Conv2d        | 5.2 K  | train\n",
      "8  | pool      | MaxPool2d     | 0      | train\n",
      "9  | up        | Upsample      | 0      | train\n",
      "10 | criterion | MaskedMSELoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "3.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 M     Total params\n",
      "12.433    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 186/186 [00:18<00:00, 10.30it/s, v_num=2, train_r2=0.701, val_r2=0.365]    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 186/186 [00:18<00:00, 10.30it/s, v_num=2, train_r2=0.701, val_r2=0.365]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 40/40 [00:10<00:00,  3.83it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.03390524163842201    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_r2          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.3613077998161316     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.03390524163842201   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test_r2         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.3613077998161316    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "# Initialize the data module\n",
    "data_module = TreeSpeciesDataModule(\n",
    "    tiles_dir=\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/rmf_s2/spring/tiles_128\",\n",
    "    labels_dir=\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/labels/tiles_128\",\n",
    "    train_tile_names=load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/train_tiles.txt\"),\n",
    "    val_tile_names=load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/val_tiles.txt\"),\n",
    "    test_tile_names=load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/test_tiles.txt\"),\n",
    "    batch_size=4,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Initialize the model\n",
    "model = UNetLightning(in_channels=9, out_channels=9, learning_rate=1e-3)\n",
    "\n",
    "# Define a checkpoint callback to save the best model\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',  # Track the validation loss\n",
    "    filename='best-model-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=1,  # Only save the best model\n",
    "    mode='min'  # We want to minimize the validation loss\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(save_dir='logs', name=\"unet_s2_loss_r2\")\n",
    "\n",
    "# Create a PyTorch Lightning Trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    logger=[csv_logger],\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "# Test the model after training\n",
    "trainer.test(model, data_module)\n",
    "\n",
    "# Save the best model after training\n",
    "trainer.save_checkpoint(\"final_model.ckpt\")\n",
    "# Load the saved model\n",
    "#model = UNetLightning.load_from_checkpoint(\"final_model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 10m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "# User specifies which datasets to use\n",
    "datasets_to_use = ['rmf_s2/spring/tiles_128']\n",
    "\n",
    "# Tile names for train, validation, and test\n",
    "tile_names = {\n",
    "    'train': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/10m/dataset/train_tiles.txt\"),\n",
    "    'val': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/10m/dataset/val_tiles.txt\"),\n",
    "    'test': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/10m/dataset/test_tiles.txt\")\n",
    "}\n",
    "\n",
    "# Initialize the DataModule\n",
    "data_module = TreeSpeciesDataModule(\n",
    "    tile_names=tile_names,\n",
    "    processed_dir=\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/10m\",  # Base directory where the datasets are stored\n",
    "    datasets_to_use=datasets_to_use,\n",
    "    batch_size=4,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Use the calculated input channels from the DataModule to initialize the model\n",
    "model = UNetLightning(in_channels=data_module.input_channels, learning_rate=1e-3)\n",
    "\n",
    "# Define a checkpoint callback to save the best model\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',  # Track the validation loss\n",
    "    filename='best-model-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=1,  # Only save the best model\n",
    "    mode='min'  # We want to minimize the validation loss\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(save_dir='logs', name=\"unet_s2_loss_r2\")\n",
    "\n",
    "# Create a PyTorch Lightning Trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    logger=[csv_logger],\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "# Test the model after training\n",
    "trainer.test(model, data_module)\n",
    "\n",
    "# Save the best model after training\n",
    "trainer.save_checkpoint(\"final_model.ckpt\")\n",
    "# Load the saved model\n",
    "#model = UNetLightning.load_from_checkpoint(\"final_model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### winter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type          | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0  | enc_conv0 | Conv2d        | 5.2 K  | train\n",
      "1  | enc_conv1 | Conv2d        | 73.9 K | train\n",
      "2  | enc_conv2 | Conv2d        | 295 K  | train\n",
      "3  | enc_conv3 | Conv2d        | 1.2 M  | train\n",
      "4  | dec_conv3 | Conv2d        | 1.2 M  | train\n",
      "5  | dec_conv2 | Conv2d        | 295 K  | train\n",
      "6  | dec_conv1 | Conv2d        | 73.8 K | train\n",
      "7  | dec_conv0 | Conv2d        | 5.2 K  | train\n",
      "8  | pool      | MaxPool2d     | 0      | train\n",
      "9  | up        | Upsample      | 0      | train\n",
      "10 | criterion | MaskedMSELoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "3.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 M     Total params\n",
      "12.433    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 186/186 [00:18<00:00, 10.30it/s, v_num=3, train_r2=0.252, val_r2=0.366]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 186/186 [00:18<00:00, 10.08it/s, v_num=3, train_r2=0.252, val_r2=0.366]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing DataLoader 0: 100%|██████████| 40/40 [00:04<00:00,  8.24it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.03373197466135025    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_r2          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.3628885746002197     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.03373197466135025   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test_r2         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.3628885746002197    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "# Initialize the data module\n",
    "data_module = TreeSpeciesDataModule(\n",
    "    tiles_dir=\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/rmf_s2/winter/tiles_128\",\n",
    "    labels_dir=\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/labels/tiles_128\",\n",
    "    train_tile_names=load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/train_tiles.txt\"),\n",
    "    val_tile_names=load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/val_tiles.txt\"),\n",
    "    test_tile_names=load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/test_tiles.txt\"),\n",
    "    batch_size=4,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Initialize the model\n",
    "model = UNetLightning(in_channels=9, out_channels=9, learning_rate=1e-3)\n",
    "\n",
    "# Define a checkpoint callback to save the best model\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',  # Track the validation loss\n",
    "    filename='best-model-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=1,  # Only save the best model\n",
    "    mode='min'  # We want to minimize the validation loss\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(save_dir='logs', name=\"unet_s2_loss_r2\")\n",
    "\n",
    "# Create a PyTorch Lightning Trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    logger=[csv_logger],\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "# Test the model after training\n",
    "trainer.test(model, data_module)\n",
    "\n",
    "# Save the best model after training\n",
    "trainer.save_checkpoint(\"final_model.ckpt\")\n",
    "# Load the saved model\n",
    "#model = UNetLightning.load_from_checkpoint(\"final_model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 10m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type          | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0  | enc_conv0 | Conv2d        | 7.0 K  | train\n",
      "1  | enc_conv1 | Conv2d        | 73.9 K | train\n",
      "2  | enc_conv2 | Conv2d        | 295 K  | train\n",
      "3  | enc_conv3 | Conv2d        | 1.2 M  | train\n",
      "4  | dec_conv3 | Conv2d        | 1.2 M  | train\n",
      "5  | dec_conv2 | Conv2d        | 295 K  | train\n",
      "6  | dec_conv1 | Conv2d        | 73.8 K | train\n",
      "7  | dec_conv0 | Conv2d        | 5.2 K  | train\n",
      "8  | pool      | MaxPool2d     | 0      | train\n",
      "9  | up        | Upsample      | 0      | train\n",
      "10 | criterion | MaskedMSELoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "3.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 M     Total params\n",
      "12.440    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 713/713 [01:19<00:00,  8.92it/s, v_num=42, train_r2=0.571, val_r2=0.355]    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 713/713 [01:19<00:00,  8.92it/s, v_num=42, train_r2=0.571, val_r2=0.355]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 153/153 [00:21<00:00,  7.23it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.035315658897161484    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_r2          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.36364343762397766    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.035315658897161484   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test_r2         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.36364343762397766   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "# User specifies which datasets to use\n",
    "datasets_to_use = ['rmf_s2/winter/tiles_128']\n",
    "\n",
    "# Tile names for train, validation, and test\n",
    "tile_names = {\n",
    "    'train': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/10m/dataset/train_tiles.txt\"),\n",
    "    'val': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/10m/dataset/val_tiles.txt\"),\n",
    "    'test': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/10m/dataset/test_tiles.txt\")\n",
    "}\n",
    "\n",
    "# Initialize the DataModule\n",
    "data_module = TreeSpeciesDataModule(\n",
    "    tile_names=tile_names,\n",
    "    processed_dir=\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/10m\",  # Base directory where the datasets are stored\n",
    "    datasets_to_use=datasets_to_use,\n",
    "    batch_size=4,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Use the calculated input channels from the DataModule to initialize the model\n",
    "model = UNetLightning(in_channels=data_module.input_channels, learning_rate=1e-3)\n",
    "\n",
    "# Define a checkpoint callback to save the best model\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',  # Track the validation loss\n",
    "    filename='best-model-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=1,  # Only save the best model\n",
    "    mode='min'  # We want to minimize the validation loss\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(save_dir='logs', name=\"unet_s2_loss_r2\")\n",
    "\n",
    "# Create a PyTorch Lightning Trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    logger=[csv_logger],\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "# Test the model after training\n",
    "trainer.test(model, data_module)\n",
    "\n",
    "# Save the best model after training\n",
    "trainer.save_checkpoint(\"final_model.ckpt\")\n",
    "# Load the saved model\n",
    "#model = UNetLightning.load_from_checkpoint(\"final_model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### combined dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4-season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA RTX A4000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type          | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0  | enc_conv0 | Conv2d        | 20.8 K | train\n",
      "1  | enc_conv1 | Conv2d        | 73.9 K | train\n",
      "2  | enc_conv2 | Conv2d        | 295 K  | train\n",
      "3  | enc_conv3 | Conv2d        | 1.2 M  | train\n",
      "4  | dec_conv3 | Conv2d        | 1.2 M  | train\n",
      "5  | dec_conv2 | Conv2d        | 295 K  | train\n",
      "6  | dec_conv1 | Conv2d        | 73.8 K | train\n",
      "7  | dec_conv0 | Conv2d        | 5.2 K  | train\n",
      "8  | pool      | MaxPool2d     | 0      | train\n",
      "9  | up        | Upsample      | 0      | train\n",
      "10 | criterion | MaskedMSELoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "3.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 M     Total params\n",
      "12.496    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 186/186 [00:37<00:00,  4.95it/s, v_num=21, train_r2=0.444, val_r2=0.376]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 186/186 [00:37<00:00,  4.90it/s, v_num=21, train_r2=0.444, val_r2=0.376]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 40/40 [00:06<00:00,  6.54it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.033558882772922516    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_r2          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.3726397454738617     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.033558882772922516   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test_r2         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.3726397454738617    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# User specifies which datasets to use\n",
    "datasets_to_use = ['rmf_s2/spring/tiles_128','rmf_s2/summer/tiles_128','rmf_s2/fall/tiles_128','rmf_s2/winter/tiles_128']\n",
    "\n",
    "# Tile names for train, validation, and test\n",
    "tile_names = {\n",
    "    'train': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/train_tiles.txt\"),\n",
    "    'val': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/val_tiles.txt\"),\n",
    "    'test': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/test_tiles.txt\")\n",
    "}\n",
    "\n",
    "# Initialize the DataModule\n",
    "data_module = TreeSpeciesDataModule(\n",
    "    tile_names=tile_names,\n",
    "    processed_dir=\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m\",  # Base directory where the datasets are stored\n",
    "    datasets_to_use=datasets_to_use,\n",
    "    batch_size=4,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Use the calculated input channels from the DataModule to initialize the model\n",
    "model = UNetLightning(in_channels=data_module.input_channels, learning_rate=1e-3)\n",
    "\n",
    "# Define a checkpoint callback to save the best model\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',  # Track the validation loss\n",
    "    filename='best-model-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=1,  # Only save the best model\n",
    "    mode='min'  # We want to minimize the validation loss\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(save_dir='logs', name=\"unet_s2_loss_r2\")\n",
    "\n",
    "# Create a PyTorch Lightning Trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    logger=[csv_logger],\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "# Test the model after training\n",
    "trainer.test(model, data_module)\n",
    "\n",
    "# Save the best model after training\n",
    "trainer.save_checkpoint(\"final_model.ckpt\")\n",
    "# Load the saved model\n",
    "#model = UNetLightning.load_from_checkpoint(\"final_model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 10m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type          | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0  | enc_conv0 | Conv2d        | 27.7 K | train\n",
      "1  | enc_conv1 | Conv2d        | 73.9 K | train\n",
      "2  | enc_conv2 | Conv2d        | 295 K  | train\n",
      "3  | enc_conv3 | Conv2d        | 1.2 M  | train\n",
      "4  | dec_conv3 | Conv2d        | 1.2 M  | train\n",
      "5  | dec_conv2 | Conv2d        | 295 K  | train\n",
      "6  | dec_conv1 | Conv2d        | 73.8 K | train\n",
      "7  | dec_conv0 | Conv2d        | 5.2 K  | train\n",
      "8  | pool      | MaxPool2d     | 0      | train\n",
      "9  | up        | Upsample      | 0      | train\n",
      "10 | criterion | MaskedMSELoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "3.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 M     Total params\n",
      "12.523    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 713/713 [02:57<00:00,  4.01it/s, v_num=43, train_r2=0.355, val_r2=0.442]    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 713/713 [02:57<00:00,  4.01it/s, v_num=43, train_r2=0.355, val_r2=0.442]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 153/153 [03:14<00:00,  0.79it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.030327918007969856    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_r2          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.4670899212360382     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.030327918007969856   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test_r2         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.4670899212360382    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# User specifies which datasets to use\n",
    "datasets_to_use = datasets_to_use = ['rmf_s2/spring/tiles_128','rmf_s2/summer/tiles_128','rmf_s2/fall/tiles_128','rmf_s2/winter/tiles_128']\n",
    "\n",
    "# Tile names for train, validation, and test\n",
    "tile_names = {\n",
    "    'train': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/10m/dataset/train_tiles.txt\"),\n",
    "    'val': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/10m/dataset/val_tiles.txt\"),\n",
    "    'test': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/10m/dataset/test_tiles.txt\")\n",
    "}\n",
    "\n",
    "# Initialize the DataModule\n",
    "data_module = TreeSpeciesDataModule(\n",
    "    tile_names=tile_names,\n",
    "    processed_dir=\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/10m\",  # Base directory where the datasets are stored\n",
    "    datasets_to_use=datasets_to_use,\n",
    "    batch_size=4,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Use the calculated input channels from the DataModule to initialize the model\n",
    "model = UNetLightning(in_channels=data_module.input_channels, learning_rate=1e-3)\n",
    "\n",
    "# Define a checkpoint callback to save the best model\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',  # Track the validation loss\n",
    "    filename='best-model-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=1,  # Only save the best model\n",
    "    mode='min'  # We want to minimize the validation loss\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(save_dir='logs', name=\"unet_s2_loss_r2\")\n",
    "\n",
    "# Create a PyTorch Lightning Trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    logger=[csv_logger],\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "# Test the model after training\n",
    "trainer.test(model, data_module)\n",
    "\n",
    "# Save the best model after training\n",
    "trainer.save_checkpoint(\"final_model.ckpt\")\n",
    "# Load the saved model\n",
    "#model = UNetLightning.load_from_checkpoint(\"final_model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### summer+fall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type          | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0  | enc_conv0 | Conv2d        | 13.9 K | train\n",
      "1  | enc_conv1 | Conv2d        | 73.9 K | train\n",
      "2  | enc_conv2 | Conv2d        | 295 K  | train\n",
      "3  | enc_conv3 | Conv2d        | 1.2 M  | train\n",
      "4  | dec_conv3 | Conv2d        | 1.2 M  | train\n",
      "5  | dec_conv2 | Conv2d        | 295 K  | train\n",
      "6  | dec_conv1 | Conv2d        | 73.8 K | train\n",
      "7  | dec_conv0 | Conv2d        | 5.2 K  | train\n",
      "8  | pool      | MaxPool2d     | 0      | train\n",
      "9  | up        | Upsample      | 0      | train\n",
      "10 | criterion | MaskedMSELoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "3.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 M     Total params\n",
      "12.468    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 713/713 [01:49<00:00,  6.51it/s, v_num=45, train_r2=-0.17, val_r2=0.450]    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 713/713 [01:49<00:00,  6.51it/s, v_num=45, train_r2=-0.17, val_r2=0.450]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 153/153 [01:29<00:00,  1.70it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.02935178577899933    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_r2          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.47970086336135864    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.02935178577899933   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test_r2         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.47970086336135864   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# User specifies which datasets to use\n",
    "datasets_to_use = datasets_to_use = ['rmf_s2/summer/tiles_128','rmf_s2/fall/tiles_128']\n",
    "\n",
    "# Tile names for train, validation, and test\n",
    "tile_names = {\n",
    "    'train': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/10m/dataset/train_tiles.txt\"),\n",
    "    'val': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/10m/dataset/val_tiles.txt\"),\n",
    "    'test': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/10m/dataset/test_tiles.txt\")\n",
    "}\n",
    "\n",
    "# Initialize the DataModule\n",
    "data_module = TreeSpeciesDataModule(\n",
    "    tile_names=tile_names,\n",
    "    processed_dir=\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/10m\",  # Base directory where the datasets are stored\n",
    "    datasets_to_use=datasets_to_use,\n",
    "    batch_size=4,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Use the calculated input channels from the DataModule to initialize the model\n",
    "model = UNetLightning(in_channels=data_module.input_channels, learning_rate=1e-3)\n",
    "\n",
    "# Define a checkpoint callback to save the best model\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',  # Track the validation loss\n",
    "    filename='best-model-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=1,  # Only save the best model\n",
    "    mode='min'  # We want to minimize the validation loss\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(save_dir='logs', name=\"unet_s2_loss_r2\")\n",
    "\n",
    "# Create a PyTorch Lightning Trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    logger=[csv_logger],\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "# Test the model after training\n",
    "trainer.test(model, data_module)\n",
    "\n",
    "# Save the best model after training\n",
    "trainer.save_checkpoint(\"final_model.ckpt\")\n",
    "# Load the saved model\n",
    "#model = UNetLightning.load_from_checkpoint(\"final_model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4-season + climate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type          | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0  | enc_conv0 | Conv2d        | 41.5 K | train\n",
      "1  | enc_conv1 | Conv2d        | 73.9 K | train\n",
      "2  | enc_conv2 | Conv2d        | 295 K  | train\n",
      "3  | enc_conv3 | Conv2d        | 1.2 M  | train\n",
      "4  | dec_conv3 | Conv2d        | 1.2 M  | train\n",
      "5  | dec_conv2 | Conv2d        | 295 K  | train\n",
      "6  | dec_conv1 | Conv2d        | 73.8 K | train\n",
      "7  | dec_conv0 | Conv2d        | 5.2 K  | train\n",
      "8  | pool      | MaxPool2d     | 0      | train\n",
      "9  | up        | Upsample      | 0      | train\n",
      "10 | criterion | MaskedMSELoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "3.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 M     Total params\n",
      "12.579    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 186/186 [00:43<00:00,  4.32it/s, v_num=22, train_r2=0.555, val_r2=0.363]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 186/186 [00:43<00:00,  4.32it/s, v_num=22, train_r2=0.555, val_r2=0.363]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 40/40 [00:06<00:00,  5.82it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.03362555801868439    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_r2          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.3607933819293976     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.03362555801868439   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test_r2         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.3607933819293976    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# User specifies which datasets to use\n",
    "datasets_to_use = ['rmf_s2/spring/tiles_128','rmf_s2/summer/tiles_128','rmf_s2/fall/tiles_128','rmf_s2/winter/tiles_128', 'rmf_climate/tiles_128']\n",
    "\n",
    "# Tile names for train, validation, and test\n",
    "tile_names = {\n",
    "    'train': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/train_tiles.txt\"),\n",
    "    'val': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/val_tiles.txt\"),\n",
    "    'test': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/test_tiles.txt\")\n",
    "}\n",
    "\n",
    "# Initialize the DataModule\n",
    "data_module = TreeSpeciesDataModule(\n",
    "    tile_names=tile_names,\n",
    "    processed_dir=\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m\",  # Base directory where the datasets are stored\n",
    "    datasets_to_use=datasets_to_use,\n",
    "    batch_size=4,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Use the calculated input channels from the DataModule to initialize the model\n",
    "model = UNetLightning(in_channels=data_module.input_channels, learning_rate=1e-3)\n",
    "\n",
    "# Define a checkpoint callback to save the best model\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',  # Track the validation loss\n",
    "    filename='best-model-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=1,  # Only save the best model\n",
    "    mode='min'  # We want to minimize the validation loss\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(save_dir='logs', name=\"unet_s2_loss_r2\")\n",
    "\n",
    "# Create a PyTorch Lightning Trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    logger=[csv_logger],\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "# Test the model after training\n",
    "trainer.test(model, data_module)\n",
    "\n",
    "# Save the best model after training\n",
    "trainer.save_checkpoint(\"final_model.ckpt\")\n",
    "# Load the saved model\n",
    "#model = UNetLightning.load_from_checkpoint(\"final_model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4-season + phenology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type          | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0  | enc_conv0 | Conv2d        | 21.4 K | train\n",
      "1  | enc_conv1 | Conv2d        | 73.9 K | train\n",
      "2  | enc_conv2 | Conv2d        | 295 K  | train\n",
      "3  | enc_conv3 | Conv2d        | 1.2 M  | train\n",
      "4  | dec_conv3 | Conv2d        | 1.2 M  | train\n",
      "5  | dec_conv2 | Conv2d        | 295 K  | train\n",
      "6  | dec_conv1 | Conv2d        | 73.8 K | train\n",
      "7  | dec_conv0 | Conv2d        | 5.2 K  | train\n",
      "8  | pool      | MaxPool2d     | 0      | train\n",
      "9  | up        | Upsample      | 0      | train\n",
      "10 | criterion | MaskedMSELoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "3.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 M     Total params\n",
      "12.498    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 186/186 [00:41<00:00,  4.49it/s, v_num=32, train_r2=0.713, val_r2=0.362]    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 186/186 [00:41<00:00,  4.49it/s, v_num=32, train_r2=0.713, val_r2=0.362]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 40/40 [00:06<00:00,  5.93it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.03417683392763138    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_r2          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.35868608951568604    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.03417683392763138   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test_r2         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.35868608951568604   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# User specifies which datasets to use\n",
    "datasets_to_use = ['rmf_s2/spring/tiles_128','rmf_s2/summer/tiles_128','rmf_s2/fall/tiles_128','rmf_s2/winter/tiles_128', \\\n",
    "    'rmf_phenology/tiles_128']\n",
    "\n",
    "# Tile names for train, validation, and test\n",
    "tile_names = {\n",
    "    'train': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/train_tiles.txt\"),\n",
    "    'val': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/val_tiles.txt\"),\n",
    "    'test': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/test_tiles.txt\")\n",
    "}\n",
    "\n",
    "# Initialize the DataModule\n",
    "data_module = TreeSpeciesDataModule(\n",
    "    tile_names=tile_names,\n",
    "    processed_dir=\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m\",  # Base directory where the datasets are stored\n",
    "    datasets_to_use=datasets_to_use,\n",
    "    batch_size=4,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Use the calculated input channels from the DataModule to initialize the model\n",
    "model = UNetLightning(in_channels=data_module.input_channels, learning_rate=1e-3)\n",
    "\n",
    "# Define a checkpoint callback to save the best model\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',  # Track the validation loss\n",
    "    filename='best-model-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=1,  # Only save the best model\n",
    "    mode='min'  # We want to minimize the validation loss\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(save_dir='logs', name=\"unet_s2_loss_r2\")\n",
    "\n",
    "# Create a PyTorch Lightning Trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    logger=[csv_logger],\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "# Test the model after training\n",
    "trainer.test(model, data_module)\n",
    "\n",
    "# Save the best model after training\n",
    "trainer.save_checkpoint(\"final_model.ckpt\")\n",
    "# Load the saved model\n",
    "#model = UNetLightning.load_from_checkpoint(\"final_model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4-season + ASTEM TOPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type          | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0  | enc_conv0 | Conv2d        | 21.4 K | train\n",
      "1  | enc_conv1 | Conv2d        | 73.9 K | train\n",
      "2  | enc_conv2 | Conv2d        | 295 K  | train\n",
      "3  | enc_conv3 | Conv2d        | 1.2 M  | train\n",
      "4  | dec_conv3 | Conv2d        | 1.2 M  | train\n",
      "5  | dec_conv2 | Conv2d        | 295 K  | train\n",
      "6  | dec_conv1 | Conv2d        | 73.8 K | train\n",
      "7  | dec_conv0 | Conv2d        | 5.2 K  | train\n",
      "8  | pool      | MaxPool2d     | 0      | train\n",
      "9  | up        | Upsample      | 0      | train\n",
      "10 | criterion | MaskedMSELoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "3.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 M     Total params\n",
      "12.498    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 186/186 [00:42<00:00,  4.35it/s, v_num=33, train_r2=0.435, val_r2=0.368]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 186/186 [00:42<00:00,  4.35it/s, v_num=33, train_r2=0.435, val_r2=0.368]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 40/40 [00:07<00:00,  5.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.03379865735769272    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_r2          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.36142870783805847    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.03379865735769272   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test_r2         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.36142870783805847   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# User specifies which datasets to use\n",
    "datasets_to_use = ['rmf_s2/spring/tiles_128','rmf_s2/summer/tiles_128','rmf_s2/fall/tiles_128','rmf_s2/winter/tiles_128', \\\n",
    "    'rmf_topo/tiles_128']\n",
    "\n",
    "# Tile names for train, validation, and test\n",
    "tile_names = {\n",
    "    'train': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/train_tiles.txt\"),\n",
    "    'val': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/val_tiles.txt\"),\n",
    "    'test': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/test_tiles.txt\")\n",
    "}\n",
    "\n",
    "# Initialize the DataModule\n",
    "data_module = TreeSpeciesDataModule(\n",
    "    tile_names=tile_names,\n",
    "    processed_dir=\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m\",  # Base directory where the datasets are stored\n",
    "    datasets_to_use=datasets_to_use,\n",
    "    batch_size=4,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Use the calculated input channels from the DataModule to initialize the model\n",
    "model = UNetLightning(in_channels=data_module.input_channels, learning_rate=1e-3)\n",
    "\n",
    "# Define a checkpoint callback to save the best model\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',  # Track the validation loss\n",
    "    filename='best-model-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=1,  # Only save the best model\n",
    "    mode='min'  # We want to minimize the validation loss\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(save_dir='logs', name=\"unet_s2_loss_r2\")\n",
    "\n",
    "# Create a PyTorch Lightning Trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    logger=[csv_logger],\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "# Test the model after training\n",
    "trainer.test(model, data_module)\n",
    "\n",
    "# Save the best model after training\n",
    "trainer.save_checkpoint(\"final_model.ckpt\")\n",
    "# Load the saved model\n",
    "#model = UNetLightning.load_from_checkpoint(\"final_model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4-season + slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type          | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0  | enc_conv0 | Conv2d        | 21.4 K | train\n",
      "1  | enc_conv1 | Conv2d        | 73.9 K | train\n",
      "2  | enc_conv2 | Conv2d        | 295 K  | train\n",
      "3  | enc_conv3 | Conv2d        | 1.2 M  | train\n",
      "4  | dec_conv3 | Conv2d        | 1.2 M  | train\n",
      "5  | dec_conv2 | Conv2d        | 295 K  | train\n",
      "6  | dec_conv1 | Conv2d        | 73.8 K | train\n",
      "7  | dec_conv0 | Conv2d        | 5.2 K  | train\n",
      "8  | pool      | MaxPool2d     | 0      | train\n",
      "9  | up        | Upsample      | 0      | train\n",
      "10 | criterion | MaskedMSELoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "3.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 M     Total params\n",
      "12.498    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 186/186 [00:40<00:00,  4.56it/s, v_num=34, train_r2=0.218, val_r2=0.380]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 186/186 [00:40<00:00,  4.56it/s, v_num=34, train_r2=0.218, val_r2=0.380]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 40/40 [00:06<00:00,  5.88it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.03311126306653023    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_r2          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.37440091371536255    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.03311126306653023   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test_r2         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.37440091371536255   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# User specifies which datasets to use\n",
    "datasets_to_use = ['rmf_s2/spring/tiles_128','rmf_s2/summer/tiles_128','rmf_s2/fall/tiles_128','rmf_s2/winter/tiles_128', \\\n",
    "    'rmf_slope/tiles_128']\n",
    "\n",
    "# Tile names for train, validation, and test\n",
    "tile_names = {\n",
    "    'train': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/train_tiles.txt\"),\n",
    "    'val': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/val_tiles.txt\"),\n",
    "    'test': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/test_tiles.txt\")\n",
    "}\n",
    "\n",
    "# Initialize the DataModule\n",
    "data_module = TreeSpeciesDataModule(\n",
    "    tile_names=tile_names,\n",
    "    processed_dir=\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m\",  # Base directory where the datasets are stored\n",
    "    datasets_to_use=datasets_to_use,\n",
    "    batch_size=4,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Use the calculated input channels from the DataModule to initialize the model\n",
    "model = UNetLightning(in_channels=data_module.input_channels, learning_rate=1e-3)\n",
    "\n",
    "# Define a checkpoint callback to save the best model\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',  # Track the validation loss\n",
    "    filename='best-model-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=1,  # Only save the best model\n",
    "    mode='min'  # We want to minimize the validation loss\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(save_dir='logs', name=\"unet_s2_loss_r2\")\n",
    "\n",
    "# Create a PyTorch Lightning Trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    logger=[csv_logger],\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "# Test the model after training\n",
    "trainer.test(model, data_module)\n",
    "\n",
    "# Save the best model after training\n",
    "trainer.save_checkpoint(\"final_model.ckpt\")\n",
    "# Load the saved model\n",
    "#model = UNetLightning.load_from_checkpoint(\"final_model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4-season + aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type          | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0  | enc_conv0 | Conv2d        | 21.4 K | train\n",
      "1  | enc_conv1 | Conv2d        | 73.9 K | train\n",
      "2  | enc_conv2 | Conv2d        | 295 K  | train\n",
      "3  | enc_conv3 | Conv2d        | 1.2 M  | train\n",
      "4  | dec_conv3 | Conv2d        | 1.2 M  | train\n",
      "5  | dec_conv2 | Conv2d        | 295 K  | train\n",
      "6  | dec_conv1 | Conv2d        | 73.8 K | train\n",
      "7  | dec_conv0 | Conv2d        | 5.2 K  | train\n",
      "8  | pool      | MaxPool2d     | 0      | train\n",
      "9  | up        | Upsample      | 0      | train\n",
      "10 | criterion | MaskedMSELoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "3.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 M     Total params\n",
      "12.498    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 186/186 [00:41<00:00,  4.44it/s, v_num=35, train_r2=0.202, val_r2=0.400]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 186/186 [00:42<00:00,  4.39it/s, v_num=35, train_r2=0.202, val_r2=0.400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 40/40 [00:07<00:00,  5.60it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.031800683587789536    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_r2          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.3972870409488678     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.031800683587789536   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test_r2         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.3972870409488678    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# User specifies which datasets to use\n",
    "datasets_to_use = ['rmf_s2/spring/tiles_128','rmf_s2/summer/tiles_128','rmf_s2/fall/tiles_128','rmf_s2/winter/tiles_128', \\\n",
    "    'rmf_aspect/tiles_128']\n",
    "\n",
    "# Tile names for train, validation, and test\n",
    "tile_names = {\n",
    "    'train': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/train_tiles.txt\"),\n",
    "    'val': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/val_tiles.txt\"),\n",
    "    'test': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/test_tiles.txt\")\n",
    "}\n",
    "\n",
    "# Initialize the DataModule\n",
    "data_module = TreeSpeciesDataModule(\n",
    "    tile_names=tile_names,\n",
    "    processed_dir=\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m\",  # Base directory where the datasets are stored\n",
    "    datasets_to_use=datasets_to_use,\n",
    "    batch_size=4,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Use the calculated input channels from the DataModule to initialize the model\n",
    "model = UNetLightning(in_channels=data_module.input_channels, learning_rate=1e-3)\n",
    "\n",
    "# Define a checkpoint callback to save the best model\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',  # Track the validation loss\n",
    "    filename='best-model-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=1,  # Only save the best model\n",
    "    mode='min'  # We want to minimize the validation loss\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(save_dir='logs', name=\"unet_s2_loss_r2\")\n",
    "\n",
    "# Create a PyTorch Lightning Trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    logger=[csv_logger],\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "# Test the model after training\n",
    "trainer.test(model, data_module)\n",
    "\n",
    "# Save the best model after training\n",
    "trainer.save_checkpoint(\"final_model.ckpt\")\n",
    "# Load the saved model\n",
    "#model = UNetLightning.load_from_checkpoint(\"final_model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4-season + trasp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type          | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0  | enc_conv0 | Conv2d        | 21.4 K | train\n",
      "1  | enc_conv1 | Conv2d        | 73.9 K | train\n",
      "2  | enc_conv2 | Conv2d        | 295 K  | train\n",
      "3  | enc_conv3 | Conv2d        | 1.2 M  | train\n",
      "4  | dec_conv3 | Conv2d        | 1.2 M  | train\n",
      "5  | dec_conv2 | Conv2d        | 295 K  | train\n",
      "6  | dec_conv1 | Conv2d        | 73.8 K | train\n",
      "7  | dec_conv0 | Conv2d        | 5.2 K  | train\n",
      "8  | pool      | MaxPool2d     | 0      | train\n",
      "9  | up        | Upsample      | 0      | train\n",
      "10 | criterion | MaskedMSELoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "3.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 M     Total params\n",
      "12.498    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  22%|██▏       | 41/186 [00:08<00:30,  4.80it/s, v_num=47, train_r2=nan.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuwei-linux/code/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 40/40 [00:06<00:00,  5.98it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            nan            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_r2          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            nan            </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           nan           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test_r2         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           nan           \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# User specifies which datasets to use\n",
    "datasets_to_use = ['rmf_s2/spring/tiles_128','rmf_s2/summer/tiles_128','rmf_s2/fall/tiles_128','rmf_s2/winter/tiles_128', \\\n",
    "    'rmf_twi/tiles_128']\n",
    "\n",
    "# Tile names for train, validation, and test\n",
    "tile_names = {\n",
    "    'train': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/train_tiles.txt\"),\n",
    "    'val': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/val_tiles.txt\"),\n",
    "    'test': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/test_tiles.txt\")\n",
    "}\n",
    "\n",
    "# Initialize the DataModule\n",
    "data_module = TreeSpeciesDataModule(\n",
    "    tile_names=tile_names,\n",
    "    processed_dir=\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m\",  # Base directory where the datasets are stored\n",
    "    datasets_to_use=datasets_to_use,\n",
    "    batch_size=4,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Use the calculated input channels from the DataModule to initialize the model\n",
    "model = UNetLightning(in_channels=data_module.input_channels, learning_rate=1e-3)\n",
    "\n",
    "# Define a checkpoint callback to save the best model\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',  # Track the validation loss\n",
    "    filename='best-model-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=1,  # Only save the best model\n",
    "    mode='min'  # We want to minimize the validation loss\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(save_dir='logs', name=\"unet_s2_loss_r2\")\n",
    "\n",
    "# Create a PyTorch Lightning Trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    logger=[csv_logger],\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "# Test the model after training\n",
    "trainer.test(model, data_module)\n",
    "\n",
    "# Save the best model after training\n",
    "trainer.save_checkpoint(\"final_model.ckpt\")\n",
    "# Load the saved model\n",
    "#model = UNetLightning.load_from_checkpoint(\"final_model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4-season + slope + aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type          | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0  | enc_conv0 | Conv2d        | 22.0 K | train\n",
      "1  | enc_conv1 | Conv2d        | 73.9 K | train\n",
      "2  | enc_conv2 | Conv2d        | 295 K  | train\n",
      "3  | enc_conv3 | Conv2d        | 1.2 M  | train\n",
      "4  | dec_conv3 | Conv2d        | 1.2 M  | train\n",
      "5  | dec_conv2 | Conv2d        | 295 K  | train\n",
      "6  | dec_conv1 | Conv2d        | 73.8 K | train\n",
      "7  | dec_conv0 | Conv2d        | 5.2 K  | train\n",
      "8  | pool      | MaxPool2d     | 0      | train\n",
      "9  | up        | Upsample      | 0      | train\n",
      "10 | criterion | MaskedMSELoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "3.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 M     Total params\n",
      "12.500    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 186/186 [00:43<00:00,  4.23it/s, v_num=37, train_r2=0.457, val_r2=0.371]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 186/186 [00:44<00:00,  4.20it/s, v_num=37, train_r2=0.457, val_r2=0.371]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 40/40 [00:07<00:00,  5.62it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.03380639851093292    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_r2          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.3623181879520416     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.03380639851093292   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test_r2         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.3623181879520416    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# User specifies which datasets to use\n",
    "datasets_to_use = ['rmf_s2/spring/tiles_128','rmf_s2/summer/tiles_128','rmf_s2/fall/tiles_128','rmf_s2/winter/tiles_128', \\\n",
    "    'rmf_slope/tiles_128', 'rmf_aspect/tiles_128']\n",
    "\n",
    "# Tile names for train, validation, and test\n",
    "tile_names = {\n",
    "    'train': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/train_tiles.txt\"),\n",
    "    'val': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/val_tiles.txt\"),\n",
    "    'test': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/test_tiles.txt\")\n",
    "}\n",
    "\n",
    "# Initialize the DataModule\n",
    "data_module = TreeSpeciesDataModule(\n",
    "    tile_names=tile_names,\n",
    "    processed_dir=\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m\",  # Base directory where the datasets are stored\n",
    "    datasets_to_use=datasets_to_use,\n",
    "    batch_size=4,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Use the calculated input channels from the DataModule to initialize the model\n",
    "model = UNetLightning(in_channels=data_module.input_channels, learning_rate=1e-3)\n",
    "\n",
    "# Define a checkpoint callback to save the best model\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',  # Track the validation loss\n",
    "    filename='best-model-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=1,  # Only save the best model\n",
    "    mode='min'  # We want to minimize the validation loss\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(save_dir='logs', name=\"unet_s2_loss_r2\")\n",
    "\n",
    "# Create a PyTorch Lightning Trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    logger=[csv_logger],\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "# Test the model after training\n",
    "trainer.test(model, data_module)\n",
    "\n",
    "# Save the best model after training\n",
    "trainer.save_checkpoint(\"final_model.ckpt\")\n",
    "# Load the saved model\n",
    "#model = UNetLightning.load_from_checkpoint(\"final_model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4-season + slope + aspect + topo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type          | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0  | enc_conv0 | Conv2d        | 22.5 K | train\n",
      "1  | enc_conv1 | Conv2d        | 73.9 K | train\n",
      "2  | enc_conv2 | Conv2d        | 295 K  | train\n",
      "3  | enc_conv3 | Conv2d        | 1.2 M  | train\n",
      "4  | dec_conv3 | Conv2d        | 1.2 M  | train\n",
      "5  | dec_conv2 | Conv2d        | 295 K  | train\n",
      "6  | dec_conv1 | Conv2d        | 73.8 K | train\n",
      "7  | dec_conv0 | Conv2d        | 5.2 K  | train\n",
      "8  | pool      | MaxPool2d     | 0      | train\n",
      "9  | up        | Upsample      | 0      | train\n",
      "10 | criterion | MaskedMSELoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "3.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 M     Total params\n",
      "12.503    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 186/186 [00:49<00:00,  3.79it/s, v_num=40, train_r2=0.741, val_r2=0.367]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 186/186 [00:49<00:00,  3.79it/s, v_num=40, train_r2=0.741, val_r2=0.367]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 40/40 [00:28<00:00,  1.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.034353647381067276    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_r2          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.35867494344711304    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.034353647381067276   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test_r2         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.35867494344711304   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# User specifies which datasets to use\n",
    "datasets_to_use = ['rmf_s2/spring/tiles_128','rmf_s2/summer/tiles_128','rmf_s2/fall/tiles_128', 'rmf_s2/winter/tiles_128', \\\n",
    "    'rmf_topo/tiles_128', 'rmf_slope/tiles_128', 'rmf_aspect/tiles_128']\n",
    "\n",
    "# Tile names for train, validation, and test\n",
    "tile_names = {\n",
    "    'train': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/train_tiles.txt\"),\n",
    "    'val': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/val_tiles.txt\"),\n",
    "    'test': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/test_tiles.txt\")\n",
    "}\n",
    "\n",
    "# Initialize the DataModule\n",
    "data_module = TreeSpeciesDataModule(\n",
    "    tile_names=tile_names,\n",
    "    processed_dir=\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m\",  # Base directory where the datasets are stored\n",
    "    datasets_to_use=datasets_to_use,\n",
    "    batch_size=4,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Use the calculated input channels from the DataModule to initialize the model\n",
    "model = UNetLightning(in_channels=data_module.input_channels, learning_rate=1e-3)\n",
    "\n",
    "# Define a checkpoint callback to save the best model\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',  # Track the validation loss\n",
    "    filename='best-model-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=1,  # Only save the best model\n",
    "    mode='min'  # We want to minimize the validation loss\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(save_dir='logs', name=\"unet_s2_loss_r2\")\n",
    "\n",
    "# Create a PyTorch Lightning Trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    logger=[csv_logger],\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "# Test the model after training\n",
    "trainer.test(model, data_module)\n",
    "\n",
    "# Save the best model after training\n",
    "trainer.save_checkpoint(\"final_model.ckpt\")\n",
    "# Load the saved model\n",
    "#model = UNetLightning.load_from_checkpoint(\"final_model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4-season + dem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type          | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0  | enc_conv0 | Conv2d        | 21.4 K | train\n",
      "1  | enc_conv1 | Conv2d        | 73.9 K | train\n",
      "2  | enc_conv2 | Conv2d        | 295 K  | train\n",
      "3  | enc_conv3 | Conv2d        | 1.2 M  | train\n",
      "4  | dec_conv3 | Conv2d        | 1.2 M  | train\n",
      "5  | dec_conv2 | Conv2d        | 295 K  | train\n",
      "6  | dec_conv1 | Conv2d        | 73.8 K | train\n",
      "7  | dec_conv0 | Conv2d        | 5.2 K  | train\n",
      "8  | pool      | MaxPool2d     | 0      | train\n",
      "9  | up        | Upsample      | 0      | train\n",
      "10 | criterion | MaskedMSELoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "3.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 M     Total params\n",
      "12.498    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 186/186 [00:41<00:00,  4.45it/s, v_num=36, train_r2=0.468, val_r2=0.370]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 186/186 [00:42<00:00,  4.42it/s, v_num=36, train_r2=0.468, val_r2=0.370]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 40/40 [00:07<00:00,  5.63it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.03354788199067116    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_r2          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.37181568145751953    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.03354788199067116   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test_r2         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.37181568145751953   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# User specifies which datasets to use\n",
    "datasets_to_use = ['rmf_s2/spring/tiles_128','rmf_s2/summer/tiles_128','rmf_s2/fall/tiles_128','rmf_s2/winter/tiles_128', \\\n",
    "    'rmf_dem/tiles_128']\n",
    "\n",
    "# Tile names for train, validation, and test\n",
    "tile_names = {\n",
    "    'train': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/train_tiles.txt\"),\n",
    "    'val': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/val_tiles.txt\"),\n",
    "    'test': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/test_tiles.txt\")\n",
    "}\n",
    "\n",
    "# Initialize the DataModule\n",
    "data_module = TreeSpeciesDataModule(\n",
    "    tile_names=tile_names,\n",
    "    processed_dir=\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m\",  # Base directory where the datasets are stored\n",
    "    datasets_to_use=datasets_to_use,\n",
    "    batch_size=4,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Use the calculated input channels from the DataModule to initialize the model\n",
    "model = UNetLightning(in_channels=data_module.input_channels, learning_rate=1e-3)\n",
    "\n",
    "# Define a checkpoint callback to save the best model\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',  # Track the validation loss\n",
    "    filename='best-model-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=1,  # Only save the best model\n",
    "    mode='min'  # We want to minimize the validation loss\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(save_dir='logs', name=\"unet_s2_loss_r2\")\n",
    "\n",
    "# Create a PyTorch Lightning Trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    logger=[csv_logger],\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "# Test the model after training\n",
    "trainer.test(model, data_module)\n",
    "\n",
    "# Save the best model after training\n",
    "trainer.save_checkpoint(\"final_model.ckpt\")\n",
    "# Load the saved model\n",
    "#model = UNetLightning.load_from_checkpoint(\"final_model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4-season + climate + phenology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type          | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0  | enc_conv0 | Conv2d        | 42.1 K | train\n",
      "1  | enc_conv1 | Conv2d        | 73.9 K | train\n",
      "2  | enc_conv2 | Conv2d        | 295 K  | train\n",
      "3  | enc_conv3 | Conv2d        | 1.2 M  | train\n",
      "4  | dec_conv3 | Conv2d        | 1.2 M  | train\n",
      "5  | dec_conv2 | Conv2d        | 295 K  | train\n",
      "6  | dec_conv1 | Conv2d        | 73.8 K | train\n",
      "7  | dec_conv0 | Conv2d        | 5.2 K  | train\n",
      "8  | pool      | MaxPool2d     | 0      | train\n",
      "9  | up        | Upsample      | 0      | train\n",
      "10 | criterion | MaskedMSELoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "3.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 M     Total params\n",
      "12.581    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 186/186 [00:47<00:00,  3.94it/s, v_num=23, train_r2=0.614, val_r2=0.365]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 186/186 [00:47<00:00,  3.91it/s, v_num=23, train_r2=0.614, val_r2=0.365]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 40/40 [00:07<00:00,  5.07it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.03402198478579521    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_r2          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.3584168553352356     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.03402198478579521   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test_r2         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.3584168553352356    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# User specifies which datasets to use\n",
    "datasets_to_use = ['rmf_s2/spring/tiles_128','rmf_s2/summer/tiles_128','rmf_s2/fall/tiles_128','rmf_s2/winter/tiles_128', \\\n",
    "    'rmf_climate/tiles_128', 'rmf_phenology/tiles_128']\n",
    "\n",
    "# Tile names for train, validation, and test\n",
    "tile_names = {\n",
    "    'train': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/train_tiles.txt\"),\n",
    "    'val': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/val_tiles.txt\"),\n",
    "    'test': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/test_tiles.txt\")\n",
    "}\n",
    "\n",
    "# Initialize the DataModule\n",
    "data_module = TreeSpeciesDataModule(\n",
    "    tile_names=tile_names,\n",
    "    processed_dir=\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m\",  # Base directory where the datasets are stored\n",
    "    datasets_to_use=datasets_to_use,\n",
    "    batch_size=4,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Use the calculated input channels from the DataModule to initialize the model\n",
    "model = UNetLightning(in_channels=data_module.input_channels, learning_rate=1e-3)\n",
    "\n",
    "# Define a checkpoint callback to save the best model\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',  # Track the validation loss\n",
    "    filename='best-model-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=1,  # Only save the best model\n",
    "    mode='min'  # We want to minimize the validation loss\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(save_dir='logs', name=\"unet_s2_loss_r2\")\n",
    "\n",
    "# Create a PyTorch Lightning Trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    logger=[csv_logger],\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "# Test the model after training\n",
    "trainer.test(model, data_module)\n",
    "\n",
    "# Save the best model after training\n",
    "trainer.save_checkpoint(\"final_model.ckpt\")\n",
    "# Load the saved model\n",
    "#model = UNetLightning.load_from_checkpoint(\"final_model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-season + climate + phenology + slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type          | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0  | enc_conv0 | Conv2d        | 42.7 K | train\n",
      "1  | enc_conv1 | Conv2d        | 73.9 K | train\n",
      "2  | enc_conv2 | Conv2d        | 295 K  | train\n",
      "3  | enc_conv3 | Conv2d        | 1.2 M  | train\n",
      "4  | dec_conv3 | Conv2d        | 1.2 M  | train\n",
      "5  | dec_conv2 | Conv2d        | 295 K  | train\n",
      "6  | dec_conv1 | Conv2d        | 73.8 K | train\n",
      "7  | dec_conv0 | Conv2d        | 5.2 K  | train\n",
      "8  | pool      | MaxPool2d     | 0      | train\n",
      "9  | up        | Upsample      | 0      | train\n",
      "10 | criterion | MaskedMSELoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "3.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 M     Total params\n",
      "12.583    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 186/186 [00:50<00:00,  3.68it/s, v_num=29, train_r2=0.160, val_r2=0.380]    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 186/186 [00:50<00:00,  3.68it/s, v_num=29, train_r2=0.160, val_r2=0.380]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 40/40 [00:08<00:00,  4.67it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.033166274428367615    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_r2          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.3777430057525635     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.033166274428367615   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test_r2         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.3777430057525635    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# User specifies which datasets to use\n",
    "datasets_to_use = ['rmf_s2/spring/tiles_128','rmf_s2/summer/tiles_128','rmf_s2/fall/tiles_128','rmf_s2/winter/tiles_128', \\\n",
    "    'rmf_climate/tiles_128', 'rmf_phenology/tiles_128', 'rmf_slope/tiles_128']\n",
    "\n",
    "# Tile names for train, validation, and test\n",
    "tile_names = {\n",
    "    'train': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/train_tiles.txt\"),\n",
    "    'val': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/val_tiles.txt\"),\n",
    "    'test': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/test_tiles.txt\")\n",
    "}\n",
    "\n",
    "# Initialize the DataModule\n",
    "data_module = TreeSpeciesDataModule(\n",
    "    tile_names=tile_names,\n",
    "    processed_dir=\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m\",  # Base directory where the datasets are stored\n",
    "    datasets_to_use=datasets_to_use,\n",
    "    batch_size=4,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Use the calculated input channels from the DataModule to initialize the model\n",
    "model = UNetLightning(in_channels=data_module.input_channels, learning_rate=1e-3)\n",
    "\n",
    "# Define a checkpoint callback to save the best model\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',  # Track the validation loss\n",
    "    filename='best-model-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=1,  # Only save the best model\n",
    "    mode='min'  # We want to minimize the validation loss\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(save_dir='logs', name=\"unet_s2_loss_r2\")\n",
    "\n",
    "# Create a PyTorch Lightning Trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    logger=[csv_logger],\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "# Test the model after training\n",
    "trainer.test(model, data_module)\n",
    "\n",
    "# Save the best model after training\n",
    "trainer.save_checkpoint(\"final_model.ckpt\")\n",
    "# Load the saved model\n",
    "#model = UNetLightning.load_from_checkpoint(\"final_model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4-season + climate + phenology + aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type          | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0  | enc_conv0 | Conv2d        | 42.7 K | train\n",
      "1  | enc_conv1 | Conv2d        | 73.9 K | train\n",
      "2  | enc_conv2 | Conv2d        | 295 K  | train\n",
      "3  | enc_conv3 | Conv2d        | 1.2 M  | train\n",
      "4  | dec_conv3 | Conv2d        | 1.2 M  | train\n",
      "5  | dec_conv2 | Conv2d        | 295 K  | train\n",
      "6  | dec_conv1 | Conv2d        | 73.8 K | train\n",
      "7  | dec_conv0 | Conv2d        | 5.2 K  | train\n",
      "8  | pool      | MaxPool2d     | 0      | train\n",
      "9  | up        | Upsample      | 0      | train\n",
      "10 | criterion | MaskedMSELoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "3.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 M     Total params\n",
      "12.583    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 186/186 [00:51<00:00,  3.60it/s, v_num=30, train_r2=0.424, val_r2=0.396]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 186/186 [00:52<00:00,  3.56it/s, v_num=30, train_r2=0.424, val_r2=0.396]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 40/40 [00:08<00:00,  4.82it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.03226141259074211    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_r2          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.39168646931648254    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.03226141259074211   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test_r2         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.39168646931648254   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# User specifies which datasets to use\n",
    "datasets_to_use = ['rmf_s2/spring/tiles_128','rmf_s2/summer/tiles_128','rmf_s2/fall/tiles_128','rmf_s2/winter/tiles_128', \\\n",
    "    'rmf_climate/tiles_128', 'rmf_phenology/tiles_128', 'rmf_aspect/tiles_128']\n",
    "\n",
    "# Tile names for train, validation, and test\n",
    "tile_names = {\n",
    "    'train': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/train_tiles.txt\"),\n",
    "    'val': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/val_tiles.txt\"),\n",
    "    'test': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/test_tiles.txt\")\n",
    "}\n",
    "\n",
    "# Initialize the DataModule\n",
    "data_module = TreeSpeciesDataModule(\n",
    "    tile_names=tile_names,\n",
    "    processed_dir=\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m\",  # Base directory where the datasets are stored\n",
    "    datasets_to_use=datasets_to_use,\n",
    "    batch_size=4,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Use the calculated input channels from the DataModule to initialize the model\n",
    "model = UNetLightning(in_channels=data_module.input_channels, learning_rate=1e-3)\n",
    "\n",
    "# Define a checkpoint callback to save the best model\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',  # Track the validation loss\n",
    "    filename='best-model-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=1,  # Only save the best model\n",
    "    mode='min'  # We want to minimize the validation loss\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(save_dir='logs', name=\"unet_s2_loss_r2\")\n",
    "\n",
    "# Create a PyTorch Lightning Trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    logger=[csv_logger],\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "# Test the model after training\n",
    "trainer.test(model, data_module)\n",
    "\n",
    "# Save the best model after training\n",
    "trainer.save_checkpoint(\"final_model.ckpt\")\n",
    "# Load the saved model\n",
    "#model = UNetLightning.load_from_checkpoint(\"final_model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4-season + climate + phenology + topo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type          | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0  | enc_conv0 | Conv2d        | 42.7 K | train\n",
      "1  | enc_conv1 | Conv2d        | 73.9 K | train\n",
      "2  | enc_conv2 | Conv2d        | 295 K  | train\n",
      "3  | enc_conv3 | Conv2d        | 1.2 M  | train\n",
      "4  | dec_conv3 | Conv2d        | 1.2 M  | train\n",
      "5  | dec_conv2 | Conv2d        | 295 K  | train\n",
      "6  | dec_conv1 | Conv2d        | 73.8 K | train\n",
      "7  | dec_conv0 | Conv2d        | 5.2 K  | train\n",
      "8  | pool      | MaxPool2d     | 0      | train\n",
      "9  | up        | Upsample      | 0      | train\n",
      "10 | criterion | MaskedMSELoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "3.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 M     Total params\n",
      "12.583    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 186/186 [00:49<00:00,  3.73it/s, v_num=38, train_r2=0.478, val_r2=0.387]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 186/186 [00:49<00:00,  3.73it/s, v_num=38, train_r2=0.478, val_r2=0.387]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 40/40 [00:10<00:00,  3.98it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.03283621370792389    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_r2          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.3848057985305786     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.03283621370792389   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test_r2         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.3848057985305786    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# User specifies which datasets to use\n",
    "datasets_to_use = ['rmf_s2/spring/tiles_128','rmf_s2/summer/tiles_128','rmf_s2/fall/tiles_128','rmf_s2/winter/tiles_128', \\\n",
    "    'rmf_climate/tiles_128', 'rmf_phenology/tiles_128', 'rmf_topo/tiles_128']\n",
    "\n",
    "# Tile names for train, validation, and test\n",
    "tile_names = {\n",
    "    'train': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/train_tiles.txt\"),\n",
    "    'val': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/val_tiles.txt\"),\n",
    "    'test': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/test_tiles.txt\")\n",
    "}\n",
    "\n",
    "# Initialize the DataModule\n",
    "data_module = TreeSpeciesDataModule(\n",
    "    tile_names=tile_names,\n",
    "    processed_dir=\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m\",  # Base directory where the datasets are stored\n",
    "    datasets_to_use=datasets_to_use,\n",
    "    batch_size=4,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Use the calculated input channels from the DataModule to initialize the model\n",
    "model = UNetLightning(in_channels=data_module.input_channels, learning_rate=1e-3)\n",
    "\n",
    "# Define a checkpoint callback to save the best model\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',  # Track the validation loss\n",
    "    filename='best-model-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=1,  # Only save the best model\n",
    "    mode='min'  # We want to minimize the validation loss\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(save_dir='logs', name=\"unet_s2_loss_r2\")\n",
    "\n",
    "# Create a PyTorch Lightning Trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    logger=[csv_logger],\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "# Test the model after training\n",
    "trainer.test(model, data_module)\n",
    "\n",
    "# Save the best model after training\n",
    "trainer.save_checkpoint(\"final_model.ckpt\")\n",
    "# Load the saved model\n",
    "#model = UNetLightning.load_from_checkpoint(\"final_model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### all - dem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type          | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0  | enc_conv0 | Conv2d        | 43.8 K | train\n",
      "1  | enc_conv1 | Conv2d        | 73.9 K | train\n",
      "2  | enc_conv2 | Conv2d        | 295 K  | train\n",
      "3  | enc_conv3 | Conv2d        | 1.2 M  | train\n",
      "4  | dec_conv3 | Conv2d        | 1.2 M  | train\n",
      "5  | dec_conv2 | Conv2d        | 295 K  | train\n",
      "6  | dec_conv1 | Conv2d        | 73.8 K | train\n",
      "7  | dec_conv0 | Conv2d        | 5.2 K  | train\n",
      "8  | pool      | MaxPool2d     | 0      | train\n",
      "9  | up        | Upsample      | 0      | train\n",
      "10 | criterion | MaskedMSELoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "3.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 M     Total params\n",
      "12.588    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 186/186 [00:55<00:00,  3.33it/s, v_num=28, train_r2=0.176, val_r2=0.369]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 186/186 [00:55<00:00,  3.33it/s, v_num=28, train_r2=0.176, val_r2=0.369]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 40/40 [00:34<00:00,  1.16it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.03395329415798187    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_r2          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.3579249083995819     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.03395329415798187   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test_r2         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.3579249083995819    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# User specifies which datasets to use\n",
    "datasets_to_use = ['rmf_s2/spring/tiles_128','rmf_s2/summer/tiles_128','rmf_s2/fall/tiles_128', 'rmf_s2/winter/tiles_128', \\\n",
    "    'rmf_climate/tiles_128', 'rmf_phenology/tiles_128', \\\n",
    "    'rmf_topo/tiles_128', 'rmf_slope/tiles_128', 'rmf_aspect/tiles_128']\n",
    "\n",
    "# Tile names for train, validation, and test\n",
    "tile_names = {\n",
    "    'train': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/train_tiles.txt\"),\n",
    "    'val': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/val_tiles.txt\"),\n",
    "    'test': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/test_tiles.txt\")\n",
    "}\n",
    "\n",
    "# Initialize the DataModule\n",
    "data_module = TreeSpeciesDataModule(\n",
    "    tile_names=tile_names,\n",
    "    processed_dir=\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m\",  # Base directory where the datasets are stored\n",
    "    datasets_to_use=datasets_to_use,\n",
    "    batch_size=4,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Use the calculated input channels from the DataModule to initialize the model\n",
    "model = UNetLightning(in_channels=data_module.input_channels, learning_rate=1e-3)\n",
    "\n",
    "# Define a checkpoint callback to save the best model\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',  # Track the validation loss\n",
    "    filename='best-model-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=1,  # Only save the best model\n",
    "    mode='min'  # We want to minimize the validation loss\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(save_dir='logs', name=\"unet_s2_loss_r2\")\n",
    "\n",
    "# Create a PyTorch Lightning Trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    logger=[csv_logger],\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "# Test the model after training\n",
    "trainer.test(model, data_module)\n",
    "\n",
    "# Save the best model after training\n",
    "trainer.save_checkpoint(\"final_model.ckpt\")\n",
    "# Load the saved model\n",
    "#model = UNetLightning.load_from_checkpoint(\"final_model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type          | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0  | enc_conv0 | Conv2d        | 44.4 K | train\n",
      "1  | enc_conv1 | Conv2d        | 73.9 K | train\n",
      "2  | enc_conv2 | Conv2d        | 295 K  | train\n",
      "3  | enc_conv3 | Conv2d        | 1.2 M  | train\n",
      "4  | dec_conv3 | Conv2d        | 1.2 M  | train\n",
      "5  | dec_conv2 | Conv2d        | 295 K  | train\n",
      "6  | dec_conv1 | Conv2d        | 73.8 K | train\n",
      "7  | dec_conv0 | Conv2d        | 5.2 K  | train\n",
      "8  | pool      | MaxPool2d     | 0      | train\n",
      "9  | up        | Upsample      | 0      | train\n",
      "10 | criterion | MaskedMSELoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "3.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 M     Total params\n",
      "12.590    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 186/186 [01:00<00:00,  3.08it/s, v_num=39, train_r2=0.581, val_r2=0.357]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 186/186 [01:00<00:00,  3.08it/s, v_num=39, train_r2=0.581, val_r2=0.357]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 40/40 [00:09<00:00,  4.02it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.034369852393865585    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_r2          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.35076746344566345    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.034369852393865585   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test_r2         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.35076746344566345   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# User specifies which datasets to use\n",
    "datasets_to_use = ['rmf_s2/spring/tiles_128','rmf_s2/summer/tiles_128','rmf_s2/fall/tiles_128', 'rmf_s2/winter/tiles_128', \\\n",
    "    'rmf_climate/tiles_128', 'rmf_phenology/tiles_128', \\\n",
    "    'rmf_topo/tiles_128', 'rmf_slope/tiles_128', 'rmf_aspect/tiles_128', \\\n",
    "    'rmf_dem/tiles_128']\n",
    "\n",
    "# Tile names for train, validation, and test\n",
    "tile_names = {\n",
    "    'train': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/train_tiles.txt\"),\n",
    "    'val': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/val_tiles.txt\"),\n",
    "    'test': load_tile_names(\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m/dataset/test_tiles.txt\")\n",
    "}\n",
    "\n",
    "# Initialize the DataModule\n",
    "data_module = TreeSpeciesDataModule(\n",
    "    tile_names=tile_names,\n",
    "    processed_dir=\"/mnt/d/Sync/research/tree_species_estimation/tree_dataset/rmf/processed/20m\",  # Base directory where the datasets are stored\n",
    "    datasets_to_use=datasets_to_use,\n",
    "    batch_size=4,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Use the calculated input channels from the DataModule to initialize the model\n",
    "model = UNetLightning(in_channels=data_module.input_channels, learning_rate=1e-3)\n",
    "\n",
    "# Define a checkpoint callback to save the best model\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',  # Track the validation loss\n",
    "    filename='best-model-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=1,  # Only save the best model\n",
    "    mode='min'  # We want to minimize the validation loss\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(save_dir='logs', name=\"unet_s2_loss_r2\")\n",
    "\n",
    "# Create a PyTorch Lightning Trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    logger=[csv_logger],\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "# Test the model after training\n",
    "trainer.test(model, data_module)\n",
    "\n",
    "# Save the best model after training\n",
    "trainer.save_checkpoint(\"final_model.ckpt\")\n",
    "# Load the saved model\n",
    "#model = UNetLightning.load_from_checkpoint(\"final_model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rf\n",
    "\n",
    "ref: https://github.com/shelleygoel/sentinel2-land-cover-classifier/tree/main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Key Points:\n",
    "  - X (Features): Sentinel imagery tiles stored in s2/tiles_128/ (each tile has 12 bands, size 128x128).\n",
    "  - Y (Labels): The species composition tiles stored in labels/tiles_128/ (each tile has 9 bands, size 128x128). The target for each pixel is a 9-element vector representing species proportions.\n",
    "  - Train/Validation/Test Splits: The tiles to use for training, validation, and testing are specified in train.txt, validation.txt, and test.txt.\n",
    "- Step-by-Step Implementation:\n",
    "  - Loading Data: We'll read all 1060 tiles from the directories for both input (X) and target (Y).\n",
    "  - Random Forest: We'll use RandomForestRegressor to fit the data.\n",
    "  - Training/Validation/Test Splits: These splits are defined by the .txt files.\n",
    "  - Pixel-Wise Classification: The model will predict the species proportions for each pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [01:40<00:00,  3.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of labels: (6553600, 12)\n",
      "shape of labels: (6553600, 9)\n",
      "loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:34<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of labels: (1638400, 12)\n",
      "shape of labels: (1638400, 9)\n",
      "start training...\n",
      "validating...\n",
      "Validation Mean Squared Error: 0.21201176008304376\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to load tiles (X) and labels (Y)\n",
    "def load_tile_data(tile_names, tiles_dir, labels_dir):\n",
    "    \"\"\"\n",
    "    Load the imagery (X) and label (Y) data for the given tile names.\n",
    "\n",
    "    Args:\n",
    "        tile_names (list): List of tile names to load.\n",
    "        tiles_dir (str): Directory containing the Sentinel imagery (X).\n",
    "        labels_dir (str): Directory containing the species composition labels (Y).\n",
    "\n",
    "    Returns:\n",
    "        X (np.array): Flattened feature array (pixels x 12).\n",
    "        Y (np.array): Flattened label array (pixels x 9).\n",
    "    \"\"\"\n",
    "    X_list, Y_list = [], []\n",
    "    print(\"loading data...\")\n",
    "    for tile_name in tqdm(tile_names):\n",
    "        # Define paths for the input and label tiles\n",
    "        input_tile_path = os.path.join(tiles_dir, tile_name)\n",
    "        label_tile_path = os.path.join(labels_dir, tile_name)\n",
    "        \n",
    "        # Load input (12 bands) and label (9 bands) tiles\n",
    "        with rasterio.open(input_tile_path) as src_x:\n",
    "            X = src_x.read()  # Shape: (12, 128, 128)\n",
    "\n",
    "        with rasterio.open(label_tile_path) as src_y:\n",
    "            Y = src_y.read()  # Shape: (9, 128, 128)\n",
    "\n",
    "        # Reshape to (num_pixels, num_bands)\n",
    "        X_flat = X.reshape(X.shape[0], -1).T  # Shape: (num_pixels, 12)\n",
    "        Y_flat = Y.reshape(Y.shape[0], -1).T  # Shape: (num_pixels, 9)\n",
    "\n",
    "        # Append to lists\n",
    "        X_list.append(X_flat)\n",
    "        Y_list.append(Y_flat)\n",
    "    \n",
    "    # Concatenate all tiles into a single array\n",
    "    X_all = np.vstack(X_list)\n",
    "    print(f\"shape of labels: {X_all.shape}\")\n",
    "    Y_all = np.vstack(Y_list)\n",
    "    print(f\"shape of labels: {Y_all.shape}\")\n",
    "    \n",
    "    return X_all, Y_all\n",
    "\n",
    "# Function to read the train/validation/test splits\n",
    "def load_split(file_path):\n",
    "    \"\"\"\n",
    "    Load the tile names from the train/validation/test split files.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the split .txt file.\n",
    "\n",
    "    Returns:\n",
    "        tile_names (list): List of tile names in the split.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        tile_names = file.read().splitlines()\n",
    "    return tile_names\n",
    "\n",
    "# Set up directories\n",
    "directory = r\"D:\\Sync\\research\\tree_species_estimation\\tree_dataset\\rmf\\processed\\10m\"\n",
    "tiles_dir = os.path.join(directory, \"rmf_s2\", \"summer\", \"tiles_128\")  # Directory for X\n",
    "labels_dir = os.path.join(directory, \"labels\", \"tiles_128\")  # Directory for Y\n",
    "\n",
    "# Load train/validation/test splits\n",
    "train_tile_names = load_split(os.path.join(directory, \"dataset\", \"train_tiles.txt\"))[:400]\n",
    "val_tile_names = load_split(os.path.join(directory, \"dataset\", \"val_tiles.txt\"))[:100]\n",
    "\n",
    "# Load the training data\n",
    "X_train, Y_train = load_tile_data(train_tile_names, tiles_dir, labels_dir)\n",
    "\n",
    "# Load the validation data (optional, but useful for hyperparameter tuning)\n",
    "X_val, Y_val = load_tile_data(val_tile_names, tiles_dir, labels_dir)\n",
    "\n",
    "# Initialize and train the Random Forest model\n",
    "print(\"start training...\")\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, Y_train)\n",
    "\n",
    "# Evaluate on the validation set\n",
    "print(\"validating...\")\n",
    "Y_val_pred = rf.predict(X_val)\n",
    "val_mse = mean_squared_error(Y_val, Y_val_pred)\n",
    "print(f\"Validation Mean Squared Error: {val_mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# After training the model\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, Y_train)\n",
    "\n",
    "# Save the model to a file\n",
    "model_filename = 'random_forest_model.joblib'\n",
    "joblib.dump(rf, model_filename)\n",
    "\n",
    "print(f\"Model saved as {model_filename}\")\n",
    "\n",
    "# Load the saved model\n",
    "loaded_rf = joblib.load(model_filename)\n",
    "\n",
    "# Load the testing data (for final evaluation)\n",
    "test_tile_names = load_split(os.path.join(directory, \"dataset\", \"test_tiles.txt\"))\n",
    "X_test, Y_test = load_tile_data(test_tile_names, tiles_dir, labels_dir)\n",
    "\n",
    "# Use the loaded model to make predictions\n",
    "Y_test_pred = loaded_rf.predict(X_test)\n",
    "test_mse = mean_squared_error(Y_test, Y_test_pred)\n",
    "print(f\"Test Mean Squared Error with loaded model: {test_mse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LUCinSA_helpers\n",
    "Helper functions and notebooks to interact with data on High-Performance Computing environment, designed to be used in conjunction with processing guide for remote sensing projects on Land-Use Change in Latin America:\n",
    "\n",
    "https://github.com/klwalker-sb/LUCinSA_helpers/tree/master\n",
    "\n",
    "https://klwalker-sb.github.io/LUCinLA_stac/Downloading.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fusion with pts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
